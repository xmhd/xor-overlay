From 5694e5386d32e8fe708a76177b44244d18bd8f7b Mon Sep 17 00:00:00 2001
From: Kris Van Hees <kris.van.hees@oracle.com>
Date: Thu, 8 Nov 2018 18:59:39 +0000
Subject: [PATCH 17/19] dtrace: add SDT probes

This adds a variety of SDT probes.

XXX add documentation here from the commit messages

Signed-off-by: Nick Alcock <nick.alcock@oracle.com>
Signed-off-by: Kris Van Hees <kris.van.hees@oracle.com>
Signed-off-by: Tomas Jedlicka <tomas.jedlicka@oracle.com>
Signed-off-by: Eugene Loh <eugene.loh@oracle.com>
Signed-off-by: Alan Maguire <alan.maguire@oracle.com>
Signed-off-by: David Mc Lean <david.mclean@oracle.com>
Signed-off-by: Vincent Lim <vincent.lim@oracle.com>
---
 block/bio.c                      |   7 ++
 block/blk-core.c                 |   4 +
 fs/exec.c                        |   8 ++
 fs/nfs/internal.h                |  13 +++
 fs/nfs/read.c                    |   3 +
 fs/nfs/write.c                   |   2 +
 fs/xfs/xfs_buf.c                 |  19 ++++
 include/linux/rwlock_api_smp.h   |  38 +++++++
 include/linux/spinlock_api_smp.h |  12 ++
 kernel/exit.c                    |   5 +
 kernel/fork.c                    |   3 +
 kernel/locking/mutex.c           |  52 ++++++++-
 kernel/locking/qrwlock.c         |  24 +++-
 kernel/locking/qspinlock.c       |  20 +++-
 kernel/sched/core.c              |  31 +++++-
 kernel/signal.c                  |  31 +++++-
 kernel/time/timer.c              |   3 +
 net/ipv4/ip_input.c              |  66 +++++++++--
 net/ipv4/ip_output.c             |  71 +++++++++++-
 net/ipv4/raw.c                   |  49 ++++++--
 net/ipv4/tcp.c                   |  26 +++++
 net/ipv4/tcp_input.c             |  43 +++++++
 net/ipv4/tcp_ipv4.c              | 114 ++++++++++++++++++-
 net/ipv4/tcp_minisocks.c         |  15 +++
 net/ipv4/tcp_output.c            |  29 +++++
 net/ipv4/udp.c                   |  26 ++++-
 net/ipv6/ip6_input.c             | 103 ++++++++++++++---
 net/ipv6/ip6_output.c            | 186 ++++++++++++++++++++++++++-----
 net/ipv6/mcast.c                 |  72 +++++++++---
 net/ipv6/ndisc.c                 |   9 ++
 net/ipv6/output_core.c           |   9 ++
 net/ipv6/raw.c                   |  38 ++++++-
 net/ipv6/tcp_ipv6.c              | 107 +++++++++++++++++-
 net/ipv6/udp.c                   |  26 ++++-
 34 files changed, 1153 insertions(+), 111 deletions(-)

diff --git a/block/bio.c b/block/bio.c
index fa01bef35bb1..f2cde8dc402c 100644
--- a/block/bio.c
+++ b/block/bio.c
@@ -1153,6 +1153,8 @@ int submit_bio_wait(struct bio *bio)
 	bio->bi_opf |= REQ_SYNC;
 	submit_bio(bio);
 
+	DTRACE_IO(wait__start, struct bio * : (bufinfo_t *, devinfo_t *), bio,
+		  struct file * : fileinfo_t *, NULL);
 	/* Prevent hang_check timer from firing at us during very long I/O */
 	hang_check = sysctl_hung_task_timeout_secs;
 	if (hang_check)
@@ -1161,6 +1163,8 @@ int submit_bio_wait(struct bio *bio)
 			;
 	else
 		wait_for_completion_io(&done);
+	DTRACE_IO(wait__done, struct bio * : (bufinfo_t *, devinfo_t *), bio,
+		  struct file * : fileinfo_t *, NULL);
 
 	return blk_status_to_errno(bio->bi_status);
 }
@@ -1444,6 +1448,9 @@ void bio_endio(struct bio *bio)
 	}
 
 	blk_throtl_bio_endio(bio);
+	DTRACE_IO(done, struct bio * :
+		  (bufinfo_t *, devinfo_t *), bio,
+		  struct file * : fileinfo_t *, NULL);
 	/* release cgroup info */
 	bio_uninit(bio);
 	if (bio->bi_end_io)
diff --git a/block/blk-core.c b/block/blk-core.c
index 2d53e2ff48ff..67523a6729fb 100644
--- a/block/blk-core.c
+++ b/block/blk-core.c
@@ -913,11 +913,15 @@ static noinline_for_stack bool submit_bio_checks(struct bio *bio)
 		 */
 		bio_set_flag(bio, BIO_TRACE_COMPLETION);
 	}
+	DTRACE_IO(start, struct bio * : (bufinfo_t *, devinfo_t *), bio,
+		  struct file * : fileinfo_t *, NULL);
 	return true;
 
 not_supported:
 	status = BLK_STS_NOTSUPP;
 end_io:
+	DTRACE_IO(start, struct bio * : (bufinfo_t *, devinfo_t *), bio,
+		  struct file * : fileinfo_t *, NULL);
 	bio->bi_status = status;
 	bio_endio(bio);
 	return false;
diff --git a/fs/exec.c b/fs/exec.c
index 4340b2803004..acd96c0e0e5f 100644
--- a/fs/exec.c
+++ b/fs/exec.c
@@ -64,6 +64,7 @@
 #include <linux/compat.h>
 #include <linux/vmalloc.h>
 #include <linux/io_uring.h>
+#include <linux/sdt.h>
 #include <linux/dtrace_os.h>
 
 #include <linux/uaccess.h>
@@ -1797,6 +1798,7 @@ static int bprm_execve(struct linux_binprm *bprm,
 	current->in_execve = 1;
 
 	file = do_open_execat(fd, filename, flags);
+	DTRACE_PROC(exec, char *, filename->name);
 	retval = PTR_ERR(file);
 	if (IS_ERR(file))
 		goto out_unmark;
@@ -1834,6 +1836,8 @@ static int bprm_execve(struct linux_binprm *bprm,
 	task_numa_free(current, false);
 	if (displaced)
 		put_files_struct(displaced);
+
+	DTRACE_PROC(exec__success);
 	return retval;
 
 out:
@@ -1923,6 +1927,8 @@ static int do_execveat_common(int fd, struct filename *filename,
 
 out_ret:
 	putname(filename);
+	if (retval < 0)
+		DTRACE_PROC(exec__failure, int, retval);
 	return retval;
 }
 
@@ -1976,6 +1982,8 @@ int kernel_execve(const char *kernel_filename,
 	free_bprm(bprm);
 out_ret:
 	putname(filename);
+	if (retval < 0)
+		DTRACE_PROC(exec__failure, int, retval);
 	return retval;
 }
 
diff --git a/fs/nfs/internal.h b/fs/nfs/internal.h
index 98554dd18a71..1e13fb97d642 100644
--- a/fs/nfs/internal.h
+++ b/fs/nfs/internal.h
@@ -10,6 +10,7 @@
 #include <linux/sunrpc/addr.h>
 #include <linux/nfs_page.h>
 #include <linux/wait_bit.h>
+#include <linux/sdt.h>
 
 #define NFS_SB_MASK (SB_RDONLY|SB_NOSUID|SB_NODEV|SB_NOEXEC|SB_SYNCHRONOUS)
 
@@ -832,3 +833,15 @@ static inline void nfs_set_port(struct sockaddr *sap, int *port,
 
 	rpc_set_port(sap, *port);
 }
+
+#define	DTRACE_IO_NFS(name, rw, size, inode)			\
+	if (DTRACE_IO_ENABLED(name)) {				\
+		struct bio bio __maybe_unused = {		\
+			.bi_opf = rw,				\
+			.bi_iter.bi_size = size,		\
+			.bi_iter.bi_sector = NFS_FILEID(inode),	\
+		};						\
+		DTRACE_IO(name, struct bio * : (bufinfo_t *,	\
+			  devinfo_t *), &bio,			\
+			  struct file * : fileinfo_t *, NULL);	\
+}
diff --git a/fs/nfs/read.c b/fs/nfs/read.c
index eb854f1f86e2..7049b4540f94 100644
--- a/fs/nfs/read.c
+++ b/fs/nfs/read.c
@@ -212,6 +212,8 @@ static void nfs_initiate_read(struct nfs_pgio_header *hdr,
 	struct inode *inode = hdr->inode;
 	int swap_flags = IS_SWAPFILE(inode) ? NFS_RPC_SWAPFLAGS : 0;
 
+	DTRACE_IO_NFS(start, REQ_OP_READ, hdr->args.count, hdr->inode);
+
 	task_setup_data->flags |= swap_flags;
 	rpc_ops->read_setup(hdr, msg);
 	trace_nfs_initiate_read(hdr);
@@ -243,6 +245,7 @@ static int nfs_readpage_done(struct rpc_task *task,
 			     struct inode *inode)
 {
 	int status = NFS_PROTO(inode)->read_done(task, hdr);
+	DTRACE_IO_NFS(done, REQ_OP_READ, hdr->res.count, hdr->inode);
 	if (status != 0)
 		return status;
 
diff --git a/fs/nfs/write.c b/fs/nfs/write.c
index 639c34fec04a..8bd0bd4e5b76 100644
--- a/fs/nfs/write.c
+++ b/fs/nfs/write.c
@@ -1403,6 +1403,7 @@ static void nfs_initiate_write(struct nfs_pgio_header *hdr,
 	task_setup_data->priority = priority;
 	rpc_ops->write_setup(hdr, msg, &task_setup_data->rpc_client);
 	trace_nfs_initiate_write(hdr);
+	DTRACE_IO_NFS(start, REQ_OP_WRITE, hdr->args.count, hdr->inode);
 }
 
 /* If a nfs_flush_* function fails, it should remove reqs from @head and
@@ -1562,6 +1563,7 @@ static int nfs_writeback_done(struct rpc_task *task,
 	 * depend on tighter cache coherency when writing.
 	 */
 	status = NFS_PROTO(inode)->write_done(task, hdr);
+	DTRACE_IO_NFS(done, REQ_OP_WRITE, hdr->res.count, hdr->inode);
 	if (status != 0)
 		return status;
 
diff --git a/fs/xfs/xfs_buf.c b/fs/xfs/xfs_buf.c
index 4e4cf91f4f9f..6550a3447816 100644
--- a/fs/xfs/xfs_buf.c
+++ b/fs/xfs/xfs_buf.c
@@ -52,6 +52,21 @@ static kmem_zone_t *xfs_buf_zone;
  *	  b_lock (trylock due to inversion)
  */
 
+#define	DTRACE_IO_XFS_WAIT(name, bp, is_write)				\
+	if (DTRACE_IO_ENABLED(name)) {					\
+		struct bio bio __maybe_unused = {			\
+			.bi_iter.bi_sector = (bp)->b_bn,		\
+			.bi_iter.bi_size = (bp)->b_length,		\
+			.bi_opf = is_write ?				\
+				REQ_OP_WRITE : REQ_OP_READ,		\
+			.bi_disk = (bp)->b_target->bt_bdev->bd_disk,	\
+			.bi_partno = (bp)->b_target->bt_bdev->bd_partno,\
+		};							\
+		DTRACE_IO(name, struct bio * : (bufinfo_t *,		\
+			  devinfo_t *), &bio,				\
+			  struct file * : fileinfo_t *, NULL);		\
+	}
+
 static int __xfs_buf_submit(struct xfs_buf *bp, bool wait);
 
 static inline int
@@ -1633,10 +1648,14 @@ static int
 xfs_buf_iowait(
 	struct xfs_buf	*bp)
 {
+	int orig_flags __attribute__((unused)) = bp->b_flags;
+
 	ASSERT(!(bp->b_flags & XBF_ASYNC));
 
 	trace_xfs_buf_iowait(bp, _RET_IP_);
+	DTRACE_IO_XFS_WAIT(wait__start, bp, orig_flags & XBF_WRITE);
 	wait_for_completion(&bp->b_iowait);
+	DTRACE_IO_XFS_WAIT(wait__done, bp, orig_flags & XBF_WRITE);
 	trace_xfs_buf_iowait_done(bp, _RET_IP_);
 
 	return bp->b_error;
diff --git a/include/linux/rwlock_api_smp.h b/include/linux/rwlock_api_smp.h
index abfb53ab11be..2531929ccb58 100644
--- a/include/linux/rwlock_api_smp.h
+++ b/include/linux/rwlock_api_smp.h
@@ -5,6 +5,8 @@
 # error "please don't include this file directly"
 #endif
 
+#include <linux/sdt.h>
+
 /*
  * include/linux/rwlock_api_smp.h
  *
@@ -119,6 +121,8 @@ static inline int __raw_read_trylock(rwlock_t *lock)
 	preempt_disable();
 	if (do_raw_read_trylock(lock)) {
 		rwlock_acquire_read(&lock->dep_map, 0, 1, _RET_IP_);
+		DTRACE_LOCKSTAT(rw__acquire, struct rwlock *, lock, int,
+				DTRACE_LOCKSTAT_RW_READER);
 		return 1;
 	}
 	preempt_enable();
@@ -130,6 +134,8 @@ static inline int __raw_write_trylock(rwlock_t *lock)
 	preempt_disable();
 	if (do_raw_write_trylock(lock)) {
 		rwlock_acquire(&lock->dep_map, 0, 1, _RET_IP_);
+		DTRACE_LOCKSTAT(rw__acquire, struct rwlock *, lock, int,
+				DTRACE_LOCKSTAT_RW_WRITER);
 		return 1;
 	}
 	preempt_enable();
@@ -148,6 +154,8 @@ static inline void __raw_read_lock(rwlock_t *lock)
 	preempt_disable();
 	rwlock_acquire_read(&lock->dep_map, 0, 0, _RET_IP_);
 	LOCK_CONTENDED(lock, do_raw_read_trylock, do_raw_read_lock);
+	DTRACE_LOCKSTAT(rw__acquire, struct rwlock *, lock, int,
+			DTRACE_LOCKSTAT_RW_READER);
 }
 
 static inline unsigned long __raw_read_lock_irqsave(rwlock_t *lock)
@@ -159,6 +167,8 @@ static inline unsigned long __raw_read_lock_irqsave(rwlock_t *lock)
 	rwlock_acquire_read(&lock->dep_map, 0, 0, _RET_IP_);
 	LOCK_CONTENDED_FLAGS(lock, do_raw_read_trylock, do_raw_read_lock,
 			     do_raw_read_lock_flags, &flags);
+	DTRACE_LOCKSTAT(rw__acquire, struct rwlock *, lock, int,
+			DTRACE_LOCKSTAT_RW_READER);
 	return flags;
 }
 
@@ -168,6 +178,8 @@ static inline void __raw_read_lock_irq(rwlock_t *lock)
 	preempt_disable();
 	rwlock_acquire_read(&lock->dep_map, 0, 0, _RET_IP_);
 	LOCK_CONTENDED(lock, do_raw_read_trylock, do_raw_read_lock);
+	DTRACE_LOCKSTAT(rw__acquire, struct rwlock *, lock, int,
+			DTRACE_LOCKSTAT_RW_READER);
 }
 
 static inline void __raw_read_lock_bh(rwlock_t *lock)
@@ -175,6 +187,8 @@ static inline void __raw_read_lock_bh(rwlock_t *lock)
 	__local_bh_disable_ip(_RET_IP_, SOFTIRQ_LOCK_OFFSET);
 	rwlock_acquire_read(&lock->dep_map, 0, 0, _RET_IP_);
 	LOCK_CONTENDED(lock, do_raw_read_trylock, do_raw_read_lock);
+	DTRACE_LOCKSTAT(rw__acquire, struct rwlock *, lock, int,
+			DTRACE_LOCKSTAT_RW_READER);
 }
 
 static inline unsigned long __raw_write_lock_irqsave(rwlock_t *lock)
@@ -186,6 +200,8 @@ static inline unsigned long __raw_write_lock_irqsave(rwlock_t *lock)
 	rwlock_acquire(&lock->dep_map, 0, 0, _RET_IP_);
 	LOCK_CONTENDED_FLAGS(lock, do_raw_write_trylock, do_raw_write_lock,
 			     do_raw_write_lock_flags, &flags);
+	DTRACE_LOCKSTAT(rw__acquire, struct rwlock *, lock, int,
+			DTRACE_LOCKSTAT_RW_WRITER);
 	return flags;
 }
 
@@ -195,6 +211,8 @@ static inline void __raw_write_lock_irq(rwlock_t *lock)
 	preempt_disable();
 	rwlock_acquire(&lock->dep_map, 0, 0, _RET_IP_);
 	LOCK_CONTENDED(lock, do_raw_write_trylock, do_raw_write_lock);
+	DTRACE_LOCKSTAT(rw__acquire, struct rwlock *, lock, int,
+			DTRACE_LOCKSTAT_RW_WRITER);
 }
 
 static inline void __raw_write_lock_bh(rwlock_t *lock)
@@ -202,6 +220,8 @@ static inline void __raw_write_lock_bh(rwlock_t *lock)
 	__local_bh_disable_ip(_RET_IP_, SOFTIRQ_LOCK_OFFSET);
 	rwlock_acquire(&lock->dep_map, 0, 0, _RET_IP_);
 	LOCK_CONTENDED(lock, do_raw_write_trylock, do_raw_write_lock);
+	DTRACE_LOCKSTAT(rw__acquire, struct rwlock *, lock, int,
+			DTRACE_LOCKSTAT_RW_WRITER);
 }
 
 static inline void __raw_write_lock(rwlock_t *lock)
@@ -209,6 +229,8 @@ static inline void __raw_write_lock(rwlock_t *lock)
 	preempt_disable();
 	rwlock_acquire(&lock->dep_map, 0, 0, _RET_IP_);
 	LOCK_CONTENDED(lock, do_raw_write_trylock, do_raw_write_lock);
+	DTRACE_LOCKSTAT(rw__acquire, struct rwlock *, lock, int,
+			DTRACE_LOCKSTAT_RW_WRITER);
 }
 
 #endif /* !CONFIG_GENERIC_LOCKBREAK || CONFIG_DEBUG_LOCK_ALLOC */
@@ -217,6 +239,8 @@ static inline void __raw_write_unlock(rwlock_t *lock)
 {
 	rwlock_release(&lock->dep_map, _RET_IP_);
 	do_raw_write_unlock(lock);
+	DTRACE_LOCKSTAT(rw__release, struct rwlock *, lock, int,
+			DTRACE_LOCKSTAT_RW_WRITER);
 	preempt_enable();
 }
 
@@ -224,6 +248,8 @@ static inline void __raw_read_unlock(rwlock_t *lock)
 {
 	rwlock_release(&lock->dep_map, _RET_IP_);
 	do_raw_read_unlock(lock);
+	DTRACE_LOCKSTAT(rw__release, struct rwlock *, lock, int,
+			DTRACE_LOCKSTAT_RW_READER);
 	preempt_enable();
 }
 
@@ -232,6 +258,8 @@ __raw_read_unlock_irqrestore(rwlock_t *lock, unsigned long flags)
 {
 	rwlock_release(&lock->dep_map, _RET_IP_);
 	do_raw_read_unlock(lock);
+	DTRACE_LOCKSTAT(rw__release, struct rwlock *, lock, int,
+			DTRACE_LOCKSTAT_RW_READER);
 	local_irq_restore(flags);
 	preempt_enable();
 }
@@ -240,6 +268,8 @@ static inline void __raw_read_unlock_irq(rwlock_t *lock)
 {
 	rwlock_release(&lock->dep_map, _RET_IP_);
 	do_raw_read_unlock(lock);
+	DTRACE_LOCKSTAT(rw__release, struct rwlock *, lock, int,
+			DTRACE_LOCKSTAT_RW_READER);
 	local_irq_enable();
 	preempt_enable();
 }
@@ -248,6 +278,8 @@ static inline void __raw_read_unlock_bh(rwlock_t *lock)
 {
 	rwlock_release(&lock->dep_map, _RET_IP_);
 	do_raw_read_unlock(lock);
+	DTRACE_LOCKSTAT(rw__release, struct rwlock *, lock, int,
+			DTRACE_LOCKSTAT_RW_READER);
 	__local_bh_enable_ip(_RET_IP_, SOFTIRQ_LOCK_OFFSET);
 }
 
@@ -256,6 +288,8 @@ static inline void __raw_write_unlock_irqrestore(rwlock_t *lock,
 {
 	rwlock_release(&lock->dep_map, _RET_IP_);
 	do_raw_write_unlock(lock);
+	DTRACE_LOCKSTAT(rw__release, struct rwlock *, lock, int,
+			DTRACE_LOCKSTAT_RW_WRITER);
 	local_irq_restore(flags);
 	preempt_enable();
 }
@@ -264,6 +298,8 @@ static inline void __raw_write_unlock_irq(rwlock_t *lock)
 {
 	rwlock_release(&lock->dep_map, _RET_IP_);
 	do_raw_write_unlock(lock);
+	DTRACE_LOCKSTAT(rw__release, struct rwlock *, lock, int,
+			DTRACE_LOCKSTAT_RW_WRITER);
 	local_irq_enable();
 	preempt_enable();
 }
@@ -272,6 +308,8 @@ static inline void __raw_write_unlock_bh(rwlock_t *lock)
 {
 	rwlock_release(&lock->dep_map, _RET_IP_);
 	do_raw_write_unlock(lock);
+	DTRACE_LOCKSTAT(rw__release, struct rwlock *, lock, int,
+			DTRACE_LOCKSTAT_RW_WRITER);
 	__local_bh_enable_ip(_RET_IP_, SOFTIRQ_LOCK_OFFSET);
 }
 
diff --git a/include/linux/spinlock_api_smp.h b/include/linux/spinlock_api_smp.h
index 19a9be9d97ee..e7601d01b87d 100644
--- a/include/linux/spinlock_api_smp.h
+++ b/include/linux/spinlock_api_smp.h
@@ -5,6 +5,8 @@
 # error "please don't include this file directly"
 #endif
 
+#include <linux/sdt.h>
+
 /*
  * include/linux/spinlock_api_smp.h
  *
@@ -88,6 +90,7 @@ static inline int __raw_spin_trylock(raw_spinlock_t *lock)
 	preempt_disable();
 	if (do_raw_spin_trylock(lock)) {
 		spin_acquire(&lock->dep_map, 0, 1, _RET_IP_);
+		DTRACE_LOCKSTAT(spin__acquire, spinlock_t *, lock);
 		return 1;
 	}
 	preempt_enable();
@@ -118,6 +121,7 @@ static inline unsigned long __raw_spin_lock_irqsave(raw_spinlock_t *lock)
 #else
 	do_raw_spin_lock_flags(lock, &flags);
 #endif
+	DTRACE_LOCKSTAT(spin__acquire, spinlock_t *, lock);
 	return flags;
 }
 
@@ -127,6 +131,7 @@ static inline void __raw_spin_lock_irq(raw_spinlock_t *lock)
 	preempt_disable();
 	spin_acquire(&lock->dep_map, 0, 0, _RET_IP_);
 	LOCK_CONTENDED(lock, do_raw_spin_trylock, do_raw_spin_lock);
+	DTRACE_LOCKSTAT(spin__acquire, spinlock_t *, lock);
 }
 
 static inline void __raw_spin_lock_bh(raw_spinlock_t *lock)
@@ -134,6 +139,7 @@ static inline void __raw_spin_lock_bh(raw_spinlock_t *lock)
 	__local_bh_disable_ip(_RET_IP_, SOFTIRQ_LOCK_OFFSET);
 	spin_acquire(&lock->dep_map, 0, 0, _RET_IP_);
 	LOCK_CONTENDED(lock, do_raw_spin_trylock, do_raw_spin_lock);
+	DTRACE_LOCKSTAT(spin__acquire, spinlock_t *, lock);
 }
 
 static inline void __raw_spin_lock(raw_spinlock_t *lock)
@@ -141,6 +147,7 @@ static inline void __raw_spin_lock(raw_spinlock_t *lock)
 	preempt_disable();
 	spin_acquire(&lock->dep_map, 0, 0, _RET_IP_);
 	LOCK_CONTENDED(lock, do_raw_spin_trylock, do_raw_spin_lock);
+	DTRACE_LOCKSTAT(spin__acquire, spinlock_t *, lock);
 }
 
 #endif /* !CONFIG_GENERIC_LOCKBREAK || CONFIG_DEBUG_LOCK_ALLOC */
@@ -149,6 +156,7 @@ static inline void __raw_spin_unlock(raw_spinlock_t *lock)
 {
 	spin_release(&lock->dep_map, _RET_IP_);
 	do_raw_spin_unlock(lock);
+	DTRACE_LOCKSTAT(spin__release, spinlock_t *, lock);
 	preempt_enable();
 }
 
@@ -157,6 +165,7 @@ static inline void __raw_spin_unlock_irqrestore(raw_spinlock_t *lock,
 {
 	spin_release(&lock->dep_map, _RET_IP_);
 	do_raw_spin_unlock(lock);
+	DTRACE_LOCKSTAT(spin__release, spinlock_t *, lock);
 	local_irq_restore(flags);
 	preempt_enable();
 }
@@ -165,6 +174,7 @@ static inline void __raw_spin_unlock_irq(raw_spinlock_t *lock)
 {
 	spin_release(&lock->dep_map, _RET_IP_);
 	do_raw_spin_unlock(lock);
+	DTRACE_LOCKSTAT(spin__release, spinlock_t *, lock);
 	local_irq_enable();
 	preempt_enable();
 }
@@ -173,6 +183,7 @@ static inline void __raw_spin_unlock_bh(raw_spinlock_t *lock)
 {
 	spin_release(&lock->dep_map, _RET_IP_);
 	do_raw_spin_unlock(lock);
+	DTRACE_LOCKSTAT(spin__release, spinlock_t *, lock);
 	__local_bh_enable_ip(_RET_IP_, SOFTIRQ_LOCK_OFFSET);
 }
 
@@ -181,6 +192,7 @@ static inline int __raw_spin_trylock_bh(raw_spinlock_t *lock)
 	__local_bh_disable_ip(_RET_IP_, SOFTIRQ_LOCK_OFFSET);
 	if (do_raw_spin_trylock(lock)) {
 		spin_acquire(&lock->dep_map, 0, 1, _RET_IP_);
+		DTRACE_LOCKSTAT(spin__acquire, spinlock_t *, lock);
 		return 1;
 	}
 	__local_bh_enable_ip(_RET_IP_, SOFTIRQ_LOCK_OFFSET);
diff --git a/kernel/exit.c b/kernel/exit.c
index da498c5f029c..285b0c0ff00d 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -64,6 +64,7 @@
 #include <linux/rcuwait.h>
 #include <linux/compat.h>
 #include <linux/io_uring.h>
+#include <linux/sdt.h>
 #include <linux/dtrace_os.h>
 
 #include <linux/uaccess.h>
@@ -796,6 +797,10 @@ void __noreturn do_exit(long code)
 	tsk->exit_code = code;
 	taskstats_exit(tsk, group_dead);
 
+	DTRACE_PROC(lwp__exit);
+	if (group_dead)
+		DTRACE_PROC(exit, int, code & 0x80 ? 3 : code & 0x7f ? 2 : 1);
+
 	/* Remove DTrace state for this task */
 	dtrace_task_free(tsk);
 
diff --git a/kernel/fork.c b/kernel/fork.c
index 0d8a2b12fd90..0729e80e4ad8 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -96,6 +96,7 @@
 #include <linux/kasan.h>
 #include <linux/scs.h>
 #include <linux/io_uring.h>
+#include <linux/sdt.h>
 #include <linux/dtrace_task_impl.h>
 
 #include <asm/pgalloc.h>
@@ -2513,6 +2514,8 @@ pid_t kernel_clone(struct kernel_clone_args *args)
 	}
 
 	put_pid(pid);
+	DTRACE_PROC(lwp__create, struct task_struct * : (lwpsinfo_t *, psinfo_t *), p);
+	DTRACE_PROC(create, struct task_struct * : psinfo_t *, p);
 	return nr;
 }
 
diff --git a/kernel/locking/mutex.c b/kernel/locking/mutex.c
index 5352ce50a97e..e784dd89d924 100644
--- a/kernel/locking/mutex.c
+++ b/kernel/locking/mutex.c
@@ -29,6 +29,7 @@
 #include <linux/interrupt.h>
 #include <linux/debug_locks.h>
 #include <linux/osq_lock.h>
+#include <linux/sdt.h>
 
 #ifdef CONFIG_DEBUG_MUTEXES
 # include "mutex-debug.h"
@@ -282,6 +283,7 @@ void __sched mutex_lock(struct mutex *lock)
 
 	if (!__mutex_trylock_fast(lock))
 		__mutex_lock_slowpath(lock);
+	DTRACE_LOCKSTAT(adaptive__acquire, struct mutex *, lock);
 }
 EXPORT_SYMBOL(mutex_lock);
 #endif
@@ -734,10 +736,14 @@ static noinline void __sched __mutex_unlock_slowpath(struct mutex *lock, unsigne
 void __sched mutex_unlock(struct mutex *lock)
 {
 #ifndef CONFIG_DEBUG_LOCK_ALLOC
-	if (__mutex_unlock_fast(lock))
+	if (__mutex_unlock_fast(lock)) {
+		DTRACE_LOCKSTAT(adaptive__release, struct mutex *, lock);
 		return;
+	}
 #endif
+
 	__mutex_unlock_slowpath(lock, _RET_IP_);
+	DTRACE_LOCKSTAT(adaptive__release, struct mutex *, lock);
 }
 EXPORT_SYMBOL(mutex_unlock);
 
@@ -927,6 +933,8 @@ __mutex_lock_common(struct mutex *lock, long state, unsigned int subclass,
 		    struct lockdep_map *nest_lock, unsigned long ip,
 		    struct ww_acquire_ctx *ww_ctx, const bool use_ww_ctx)
 {
+	u64 spinstart = 0, spinend, spintotal = 0;
+	u64 waitstart, waitend, waittotal = 0;
 	struct mutex_waiter waiter;
 	bool first = false;
 	struct ww_mutex *ww;
@@ -958,9 +966,11 @@ __mutex_lock_common(struct mutex *lock, long state, unsigned int subclass,
 	if (__mutex_trylock(lock) ||
 	    mutex_optimistic_spin(lock, ww_ctx, use_ww_ctx, NULL)) {
 		/* got the lock, yay! */
+
 		lock_acquired(&lock->dep_map, ip);
 		if (use_ww_ctx && ww_ctx)
 			ww_mutex_set_context_fastpath(ww, ww_ctx);
+		DTRACE_LOCKSTAT(adaptive__acquire, struct mutex *, lock);
 		preempt_enable();
 		return 0;
 	}
@@ -1003,6 +1013,9 @@ __mutex_lock_common(struct mutex *lock, long state, unsigned int subclass,
 	waiter.task = current;
 
 	set_current_state(state);
+	if (DTRACE_LOCKSTAT_ENABLED(adaptive__spin))
+		spinstart = dtrace_gethrtime_ns();
+
 	for (;;) {
 		/*
 		 * Once we hold wait_lock, we're serialized against
@@ -1030,7 +1043,15 @@ __mutex_lock_common(struct mutex *lock, long state, unsigned int subclass,
 		}
 
 		spin_unlock(&lock->wait_lock);
-		schedule_preempt_disabled();
+
+		if (DTRACE_LOCKSTAT_ENABLED(adaptive__block)) {
+			waitstart = dtrace_gethrtime_ns();
+			schedule_preempt_disabled();
+			waitend = dtrace_gethrtime_ns();
+			if (waitend > waitstart)
+				waittotal += waitend - waitstart;
+		} else
+			schedule_preempt_disabled();
 
 		/*
 		 * ww_mutex needs to always recheck its position since its waiter
@@ -1082,6 +1103,19 @@ __mutex_lock_common(struct mutex *lock, long state, unsigned int subclass,
 		ww_mutex_lock_acquired(ww, ww_ctx);
 
 	spin_unlock(&lock->wait_lock);
+
+	if (DTRACE_LOCKSTAT_ENABLED(adaptive__spin) && spinstart) {
+		spinend = dtrace_gethrtime_ns();
+		spintotal = (spinend > spinstart) ? (spinend - spinstart) : 0;
+		spintotal = (spintotal > waittotal) ?
+			(spintotal - waittotal) : 0;
+		DTRACE_LOCKSTAT(adaptive__spin, struct mutex *, lock,
+				uint64_t, spintotal);
+	}
+	if (DTRACE_LOCKSTAT_ENABLED(adaptive__block) && waittotal)
+		DTRACE_LOCKSTAT(adaptive__block, struct mutex *, lock,
+				uint64_t, waittotal);
+	DTRACE_LOCKSTAT(adaptive__acquire, struct mutex *, lock);
 	preempt_enable();
 	return 0;
 
@@ -1092,6 +1126,8 @@ __mutex_lock_common(struct mutex *lock, long state, unsigned int subclass,
 	spin_unlock(&lock->wait_lock);
 	debug_mutex_free_waiter(&waiter);
 	mutex_release(&lock->dep_map, ip);
+	DTRACE_LOCKSTAT(adaptive__acquire__error, struct mutex *, lock,
+			int, ret);
 	preempt_enable();
 	return ret;
 }
@@ -1307,8 +1343,10 @@ int __sched mutex_lock_interruptible(struct mutex *lock)
 {
 	might_sleep();
 
-	if (__mutex_trylock_fast(lock))
+	if (__mutex_trylock_fast(lock)) {
+		DTRACE_LOCKSTAT(adaptive__acquire, struct mutex *, lock);
 		return 0;
+	}
 
 	return __mutex_lock_interruptible_slowpath(lock);
 }
@@ -1331,8 +1369,10 @@ int __sched mutex_lock_killable(struct mutex *lock)
 {
 	might_sleep();
 
-	if (__mutex_trylock_fast(lock))
+	if (__mutex_trylock_fast(lock)) {
+		DTRACE_LOCKSTAT(adaptive__acquire, struct mutex *, lock);
 		return 0;
+	}
 
 	return __mutex_lock_killable_slowpath(lock);
 }
@@ -1416,8 +1456,10 @@ int __sched mutex_trylock(struct mutex *lock)
 #endif
 
 	locked = __mutex_trylock(lock);
-	if (locked)
+	if (locked) {
 		mutex_acquire(&lock->dep_map, 0, 1, _RET_IP_);
+		DTRACE_LOCKSTAT(adaptive__acquire, struct mutex *, lock);
+	}
 
 	return locked;
 }
diff --git a/kernel/locking/qrwlock.c b/kernel/locking/qrwlock.c
index fe9ca92faa2a..6b1511be7805 100644
--- a/kernel/locking/qrwlock.c
+++ b/kernel/locking/qrwlock.c
@@ -11,6 +11,7 @@
 #include <linux/cpumask.h>
 #include <linux/percpu.h>
 #include <linux/hardirq.h>
+#include <linux/sdt.h>
 #include <linux/spinlock.h>
 #include <asm/qrwlock.h>
 
@@ -20,9 +21,13 @@
  */
 void queued_read_lock_slowpath(struct qrwlock *lock)
 {
+	u64 spinstart = 0, spinend, spintime;
+
 	/*
 	 * Readers come here when they cannot get the lock without waiting
 	 */
+	if (DTRACE_LOCKSTAT_ENABLED(rw__spin))
+		spinstart = dtrace_gethrtime_ns();
 	if (unlikely(in_interrupt())) {
 		/*
 		 * Readers in interrupt context will get the lock immediately
@@ -31,7 +36,7 @@ void queued_read_lock_slowpath(struct qrwlock *lock)
 		 * without waiting in the queue.
 		 */
 		atomic_cond_read_acquire(&lock->cnts, !(VAL & _QW_LOCKED));
-		return;
+		goto done;
 	}
 	atomic_sub(_QR_BIAS, &lock->cnts);
 
@@ -52,6 +57,13 @@ void queued_read_lock_slowpath(struct qrwlock *lock)
 	 * Signal the next one in queue to become queue head
 	 */
 	arch_spin_unlock(&lock->wait_lock);
+done:
+	if (DTRACE_LOCKSTAT_ENABLED(rw__spin) && spinstart) {
+		spinend = dtrace_gethrtime_ns();
+		spintime = spinend > spinstart ? spinend - spinstart : 0;
+		DTRACE_LOCKSTAT(rw__spin, rwlock_t *, lock, uint64_t, spintime,
+				int, DTRACE_LOCKSTAT_RW_READER);
+	}
 }
 EXPORT_SYMBOL(queued_read_lock_slowpath);
 
@@ -61,7 +73,11 @@ EXPORT_SYMBOL(queued_read_lock_slowpath);
  */
 void queued_write_lock_slowpath(struct qrwlock *lock)
 {
+	u64 spinstart = 0, spinend, spintime;
+
 	/* Put the writer into the wait queue */
+	if (DTRACE_LOCKSTAT_ENABLED(rw__spin))
+		spinstart = dtrace_gethrtime_ns();
 	arch_spin_lock(&lock->wait_lock);
 
 	/* Try to acquire the lock directly if no reader is present */
@@ -79,5 +95,11 @@ void queued_write_lock_slowpath(struct qrwlock *lock)
 					_QW_LOCKED) != _QW_WAITING);
 unlock:
 	arch_spin_unlock(&lock->wait_lock);
+	if (DTRACE_LOCKSTAT_ENABLED(rw__spin) && spinstart) {
+		spinend = dtrace_gethrtime_ns();
+		spintime = spinend > spinstart ? spinend - spinstart : 0;
+		DTRACE_LOCKSTAT(rw__spin, rwlock_t *, lock, uint64_t, spintime,
+				int, DTRACE_LOCKSTAT_RW_WRITER);
+	}
 }
 EXPORT_SYMBOL(queued_write_lock_slowpath);
diff --git a/kernel/locking/qspinlock.c b/kernel/locking/qspinlock.c
index cbff6ba53d56..44853c2b0183 100644
--- a/kernel/locking/qspinlock.c
+++ b/kernel/locking/qspinlock.c
@@ -20,6 +20,7 @@
 #include <linux/hardirq.h>
 #include <linux/mutex.h>
 #include <linux/prefetch.h>
+#include <linux/sdt.h>
 #include <asm/byteorder.h>
 #include <asm/qspinlock.h>
 
@@ -315,16 +316,20 @@ static __always_inline u32  __pv_wait_head_or_lock(struct qspinlock *lock,
 void queued_spin_lock_slowpath(struct qspinlock *lock, u32 val)
 {
 	struct mcs_spinlock *prev, *next, *node;
+	u64 spinstart = 0, spinend, spintime;
 	u32 old, tail;
 	int idx;
 
 	BUILD_BUG_ON(CONFIG_NR_CPUS >= (1U << _Q_TAIL_CPU_BITS));
 
+	if (DTRACE_LOCKSTAT_ENABLED(spin__spin))
+		spinstart = dtrace_gethrtime_ns();
+
 	if (pv_enabled())
 		goto pv_queue;
 
 	if (virt_spin_lock(lock))
-		return;
+		goto out;
 
 	/*
 	 * Wait for in-progress pending->locked hand-overs with a bounded
@@ -388,7 +393,7 @@ void queued_spin_lock_slowpath(struct qspinlock *lock, u32 val)
 	 */
 	clear_pending_set_locked(lock);
 	lockevent_inc(lock_pending);
-	return;
+	goto out;
 
 	/*
 	 * End of pending bit optimistic spinning and beginning of MCS
@@ -558,6 +563,17 @@ void queued_spin_lock_slowpath(struct qspinlock *lock, u32 val)
 	 * release the node
 	 */
 	__this_cpu_dec(qnodes[0].mcs.count);
+
+	/*
+	 * Fire spin-spin probe to note time waiting for a lock.
+	 */
+out:
+	if (DTRACE_LOCKSTAT_ENABLED(spin__spin)) {
+		spinend = dtrace_gethrtime_ns();
+		spintime = spinend > spinstart ? spinend - spinstart : 0;
+		DTRACE_LOCKSTAT(spin__spin, spinlock_t *, lock,
+				uint64_t, spintime);
+	}
 }
 EXPORT_SYMBOL(queued_spin_lock_slowpath);
 
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index db893818f3ea..f035f9859ae7 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -1561,6 +1561,9 @@ static inline void init_uclamp(void) { }
 
 static inline void enqueue_task(struct rq *rq, struct task_struct *p, int flags)
 {
+	DTRACE_SCHED(enqueue, struct task_struct * : (lwpsinfo_t *,
+						      psinfo_t *), p,
+		     cpuinfo_t *, rq->dtrace_cpu_info);
 	if (!(flags & ENQUEUE_NOCLOCK))
 		update_rq_clock(rq);
 
@@ -1575,6 +1578,10 @@ static inline void enqueue_task(struct rq *rq, struct task_struct *p, int flags)
 
 static inline void dequeue_task(struct rq *rq, struct task_struct *p, int flags)
 {
+	DTRACE_SCHED(dequeue, struct task_struct * : (lwpsinfo_t *,
+						      psinfo_t *), p,
+		     cpuinfo_t *, rq->dtrace_cpu_info,
+		     int, 0);
 	if (!(flags & DEQUEUE_NOCLOCK))
 		update_rq_clock(rq);
 
@@ -2864,6 +2871,8 @@ try_to_wake_up(struct task_struct *p, unsigned int state, int wake_flags)
 		goto unlock;
 
 	trace_sched_waking(p);
+	DTRACE_SCHED(wakeup, struct task_struct * : (lwpsinfo_t *,
+						     psinfo_t *), p);
 
 	/* We're going to change ->state: */
 	success = 1;
@@ -3552,6 +3561,8 @@ prepare_task_switch(struct rq *rq, struct task_struct *prev,
 	sched_info_switch(rq, prev, next);
 	perf_event_task_sched_out(prev, next);
 	rseq_preempt(prev);
+	DTRACE_SCHED(off__cpu, struct task_struct * : (lwpsinfo_t *,
+						       psinfo_t *), next);
 	fire_sched_out_preempt_notifiers(prev, next);
 	prepare_task(next);
 	prepare_arch_switch(next);
@@ -3625,6 +3636,7 @@ static struct rq *finish_task_switch(struct task_struct *prev)
 	finish_arch_post_lock_switch();
 	kcov_finish_switch(current);
 
+	DTRACE_SCHED(on__cpu);
 	fire_sched_in_preempt_notifiers(current);
 	/*
 	 * When switching through a kernel thread, the loop in
@@ -3725,6 +3737,8 @@ asmlinkage __visible void schedule_tail(struct task_struct *prev)
 		put_user(task_pid_vnr(current), current->set_child_tid);
 
 	calculate_sigpending();
+	DTRACE_PROC(start);
+	DTRACE_PROC(lwp__start);
 }
 
 /*
@@ -4467,6 +4481,7 @@ static void __sched notrace __schedule(bool preempt)
 	 */
 	prev_state = prev->state;
 	if (!preempt && prev_state) {
+		DTRACE_SCHED(sleep);
 		if (signal_pending_state(prev_state, prev)) {
 			prev->state = TASK_RUNNING;
 		} else {
@@ -4497,7 +4512,8 @@ static void __sched notrace __schedule(bool preempt)
 			}
 		}
 		switch_count = &prev->nvcsw;
-	}
+	} else
+		DTRACE_SCHED(preempt);
 
 	next = pick_next_task(rq, prev, &rf);
 	clear_tsk_need_resched(prev);
@@ -4534,6 +4550,7 @@ static void __sched notrace __schedule(bool preempt)
 		rq = context_switch(rq, prev, next, &rf);
 	} else {
 		rq->clock_update_flags &= ~(RQCF_ACT_SKIP|RQCF_REQ_SKIP);
+		DTRACE_SCHED(remain__cpu);
 		rq_unlock_irq(rq, &rf);
 	}
 
@@ -4999,6 +5016,9 @@ void set_user_nice(struct task_struct *p, long nice)
 	old_prio = p->prio;
 	p->prio = effective_prio(p);
 
+	DTRACE_SCHED(change__pri, struct task_struct * : (lwpsinfo_t *,
+							  psinfo_t *), p,
+		      int, old_prio);
 	if (queued)
 		enqueue_task(rq, p, ENQUEUE_RESTORE | ENQUEUE_NOCLOCK);
 	if (running)
@@ -6110,6 +6130,9 @@ static void do_sched_yield(void)
 	rq_unlock_irq(rq, &rf);
 	sched_preempt_enable_no_resched();
 
+	DTRACE_SCHED(surrender,
+		     struct task_struct * : (lwpsinfo_t *, psinfo_t *),
+		     current);
 	schedule();
 }
 
@@ -6256,8 +6279,12 @@ int __sched yield_to(struct task_struct *p, bool preempt)
 out_irq:
 	local_irq_restore(flags);
 
-	if (yielded > 0)
+	if (yielded > 0) {
+		DTRACE_SCHED(surrender,
+			     struct task_struct * : (lwpsinfo_t *, psinfo_t *),
+			     curr);
 		schedule();
+	}
 
 	return yielded;
 }
diff --git a/kernel/signal.c b/kernel/signal.c
index ef8f2a28d37c..23cefab66287 100644
--- a/kernel/signal.c
+++ b/kernel/signal.c
@@ -49,6 +49,7 @@
 
 #define CREATE_TRACE_POINTS
 #include <trace/events/signal.h>
+#include <linux/sdt.h>
 
 #include <asm/param.h>
 #include <linux/uaccess.h>
@@ -1079,8 +1080,12 @@ static int __send_signal(int sig, struct kernel_siginfo *info, struct task_struc
 	assert_spin_locked(&t->sighand->siglock);
 
 	result = TRACE_SIGNAL_IGNORED;
-	if (!prepare_signal(sig, t, force))
+	if (!prepare_signal(sig, t, force)) {
+		DTRACE_PROC(signal__discard,
+			    struct task_struct * : (lwpsinfo_t *, psinfo_t *), t,
+			    int, sig);
 		goto ret;
+	}
 
 	pending = (type != PIDTYPE_PID) ? &t->signal->shared_pending : &t->pending;
 	/*
@@ -1179,6 +1184,9 @@ static int __send_signal(int sig, struct kernel_siginfo *info, struct task_struc
 	}
 
 	complete_signal(sig, t, type);
+	DTRACE_PROC(signal__send,
+		     struct task_struct * : (lwpsinfo_t *, psinfo_t *), t,
+		     int, sig);
 ret:
 	trace_signal_generate(sig, info, t, type != PIDTYPE_PID, result);
 	return ret;
@@ -1879,6 +1887,9 @@ int send_sigqueue(struct sigqueue *q, struct pid *pid, enum pid_type type)
 	list_add_tail(&q->list, &pending->list);
 	sigaddset(&pending->signal, sig);
 	complete_signal(sig, t, type);
+	DTRACE_PROC(signal__send,
+		    struct task_struct * : (lwpsinfo_t *, psinfo_t *), t,
+		    int, sig);
 	result = TRACE_SIGNAL_DELIVERED;
 out:
 	trace_signal_generate(sig, &q->info, t, type != PIDTYPE_PID, result);
@@ -2601,6 +2612,11 @@ bool get_signal(struct ksignal *ksig)
 
 	/* Has this task already been marked for death? */
 	if (signal_group_exit(signal)) {
+		DTRACE_PROC(signal__handle,
+			    int, signal->group_exit_code
+						? signal->group_exit_code
+						: signr,
+			    siginfo_t *, NULL, void (*)(void), NULL);
 		ksig->info.si_signo = signr = SIGKILL;
 		sigdelset(&current->pending.signal, SIGKILL);
 		trace_signal_deliver(SIGKILL, SEND_SIG_NOINFO,
@@ -2658,6 +2674,15 @@ bool get_signal(struct ksignal *ksig)
 
 		ka = &sighand->action[signr-1];
 
+		DTRACE_PROC(signal__handle,
+			    int, signal->group_exit_code
+						? signal->group_exit_code
+						: signr,
+			    siginfo_t *, ksig->ka.sa.sa_handler != SIG_DFL
+						? NULL
+						: &ksig->info,
+			    void (*)(void), ksig->ka.sa.sa_handler);
+
 		/* Trace actually delivered signals. */
 		trace_signal_deliver(signr, &ksig->info, ka);
 
@@ -3498,8 +3523,10 @@ static int do_sigtimedwait(const sigset_t *which, kernel_siginfo_t *info,
 	}
 	spin_unlock_irq(&tsk->sighand->siglock);
 
-	if (sig)
+	if (sig) {
+		DTRACE_PROC(signal__clear, int, sig);
 		return sig;
+	}
 	return ret ? -EINTR : -EAGAIN;
 }
 
diff --git a/kernel/time/timer.c b/kernel/time/timer.c
index c3ad64fb9d8b..a1b822cbdb80 100644
--- a/kernel/time/timer.c
+++ b/kernel/time/timer.c
@@ -44,6 +44,7 @@
 #include <linux/slab.h>
 #include <linux/compat.h>
 #include <linux/random.h>
+#include <linux/sdt.h>
 
 #include <linux/uaccess.h>
 #include <asm/unistd.h>
@@ -1702,6 +1703,8 @@ void update_process_times(int user_tick)
 	struct task_struct *p = current;
 
 	PRANDOM_ADD_NOISE(jiffies, user_tick, p, 0);
+	DTRACE_SCHED(tick, struct task_struct * : (lwpsinfo_t *, psinfo_t *),
+		     p);
 
 	/* Note: this timer irq context must be accounted for as well. */
 	account_process_tick(p, user_tick);
diff --git a/net/ipv4/ip_input.c b/net/ipv4/ip_input.c
index b0c244af1e4d..038868b61cd8 100644
--- a/net/ipv4/ip_input.c
+++ b/net/ipv4/ip_input.c
@@ -141,6 +141,7 @@
 #include <linux/mroute.h>
 #include <linux/netlink.h>
 #include <net/dst_metadata.h>
+#include <linux/sdt.h>
 
 /*
  *	Process Router Attention IP option (RFC 2113)
@@ -239,16 +240,26 @@ static int ip_local_deliver_finish(struct net *net, struct sock *sk, struct sk_b
  */
 int ip_local_deliver(struct sk_buff *skb)
 {
+	struct iphdr *iph = ip_hdr(skb);
+
 	/*
 	 *	Reassemble IP fragments.
 	 */
 	struct net *net = dev_net(skb->dev);
 
-	if (ip_is_fragment(ip_hdr(skb))) {
+	if (ip_is_fragment(iph)) {
 		if (ip_defrag(net, skb, IP_DEFRAG_LOCAL_DELIVER))
 			return 0;
 	}
 
+	DTRACE_IP(receive,
+		  struct sk_buff * : pktinfo_t *, skb,
+		  struct sock * : csinfo_t *, skb->sk,
+		  void_ip_t * : ipinfo_t *, iph,
+		  struct net_device * : ifinfo_t *, skb->dev,
+		  struct iphdr * : ipv4info_t *, iph,
+		  struct ipv6hdr * : ipv6info_t *, NULL);
+
 	return NF_HOOK(NFPROTO_IPV4, NF_INET_LOCAL_IN,
 		       net, NULL, skb, skb->dev, NULL,
 		       ip_local_deliver_finish);
@@ -257,7 +268,8 @@ int ip_local_deliver(struct sk_buff *skb)
 static inline bool ip_rcv_options(struct sk_buff *skb, struct net_device *dev)
 {
 	struct ip_options *opt;
-	const struct iphdr *iph;
+	const struct iphdr *iph = NULL;
+	const char *dropreason;
 
 	/* It looks as overkill, because not all
 	   IP options require packet mangling.
@@ -267,6 +279,7 @@ static inline bool ip_rcv_options(struct sk_buff *skb, struct net_device *dev)
 					      --ANK (980813)
 	*/
 	if (skb_cow(skb, skb_headroom(skb))) {
+		dropreason = "copy-on-write failed";
 		__IP_INC_STATS(dev_net(dev), IPSTATS_MIB_INDISCARDS);
 		goto drop;
 	}
@@ -276,6 +289,7 @@ static inline bool ip_rcv_options(struct sk_buff *skb, struct net_device *dev)
 	opt->optlen = iph->ihl*4 - sizeof(struct iphdr);
 
 	if (ip_options_compile(dev_net(dev), opt, skb)) {
+		dropreason = "invalid options";
 		__IP_INC_STATS(dev_net(dev), IPSTATS_MIB_INHDRERRORS);
 		goto drop;
 	}
@@ -289,16 +303,28 @@ static inline bool ip_rcv_options(struct sk_buff *skb, struct net_device *dev)
 					net_info_ratelimited("source route option %pI4 -> %pI4\n",
 							     &iph->saddr,
 							     &iph->daddr);
+				dropreason = "invalid source route options";
 				goto drop;
 			}
 		}
 
-		if (ip_options_rcv_srr(skb, dev))
+		if (ip_options_rcv_srr(skb, dev)) {
+			dropreason = "invalid options";
 			goto drop;
+		}
 	}
 
 	return false;
 drop:
+	DTRACE_IP(drop__in,
+		  struct sk_buff * : pktinfo_t *, skb,
+		  struct sock * : csinfo_t *, skb->sk,
+		  void_ip_t * : ipinfo_t *, iph,
+		  struct net_device * : ifinfo_t *, skb->dev,
+		  struct iphdr * : ipv4info_t *, iph,
+		  struct ipv6hdr * : ipv6info_t *, NULL,
+		  const char * : string, dropreason);
+
 	return true;
 }
 
@@ -432,27 +458,35 @@ static int ip_rcv_finish(struct net *net, struct sock *sk, struct sk_buff *skb)
 /*
  * 	Main IP Receive routine.
  */
-static struct sk_buff *ip_rcv_core(struct sk_buff *skb, struct net *net)
+static struct sk_buff *ip_rcv_core(struct sk_buff *skb, struct net *net,
+				   struct net_device *dev
+					   __attribute__((__unused__)))
 {
-	const struct iphdr *iph;
+	const struct iphdr *iph = NULL;
 	u32 len;
+	const char *dropreason = "header invalid";
 
 	/* When the interface is in promisc. mode, drop all the crap
 	 * that it receives, do not try to analyse it.
 	 */
-	if (skb->pkt_type == PACKET_OTHERHOST)
+	if (skb->pkt_type == PACKET_OTHERHOST) {
+		dropreason = "for other host";
 		goto drop;
+	}
 
 	__IP_UPD_PO_STATS(net, IPSTATS_MIB_IN, skb->len);
 
 	skb = skb_share_check(skb, GFP_ATOMIC);
 	if (!skb) {
+		dropreason = "could not clone shared buffer";
 		__IP_INC_STATS(net, IPSTATS_MIB_INDISCARDS);
-		goto out;
+		goto drop;
 	}
 
-	if (!pskb_may_pull(skb, sizeof(struct iphdr)))
+	if (!pskb_may_pull(skb, sizeof(struct iphdr))) {
+		dropreason = "could not pull skb";
 		goto inhdr_error;
+	}
 
 	iph = ip_hdr(skb);
 
@@ -487,6 +521,7 @@ static struct sk_buff *ip_rcv_core(struct sk_buff *skb, struct net *net)
 
 	len = ntohs(iph->tot_len);
 	if (skb->len < len) {
+		dropreason = "packet too short";
 		__IP_INC_STATS(net, IPSTATS_MIB_INTRUNCATEDPKTS);
 		goto drop;
 	} else if (len < (iph->ihl*4))
@@ -497,6 +532,7 @@ static struct sk_buff *ip_rcv_core(struct sk_buff *skb, struct net *net)
 	 * Note this now means skb->len holds ntohs(iph->tot_len).
 	 */
 	if (pskb_trim_rcsum(skb, len)) {
+		dropreason = "could not trim buffer";
 		__IP_INC_STATS(net, IPSTATS_MIB_INDISCARDS);
 		goto drop;
 	}
@@ -516,11 +552,19 @@ static struct sk_buff *ip_rcv_core(struct sk_buff *skb, struct net *net)
 
 csum_error:
 	__IP_INC_STATS(net, IPSTATS_MIB_CSUMERRORS);
+	dropreason = "checksum error";
 inhdr_error:
 	__IP_INC_STATS(net, IPSTATS_MIB_INHDRERRORS);
 drop:
+	DTRACE_IP(drop__in,
+		  struct sk_buff * : pktinfo_t *, skb,
+		  struct sock * : csinfo_t *, skb ? skb->sk : NULL,
+		  void_ip_t * : ipinfo_t *, iph,
+		  struct net_device * : ifinfo_t *, dev,
+		  struct iphdr * : ipv4info_t *, iph,
+		  void * : ipv6info_t *, NULL,
+		  const char * : string, dropreason);
 	kfree_skb(skb);
-out:
 	return NULL;
 }
 
@@ -532,7 +576,7 @@ int ip_rcv(struct sk_buff *skb, struct net_device *dev, struct packet_type *pt,
 {
 	struct net *net = dev_net(dev);
 
-	skb = ip_rcv_core(skb, net);
+	skb = ip_rcv_core(skb, net, dev);
 	if (skb == NULL)
 		return NET_RX_DROP;
 
@@ -623,7 +667,7 @@ void ip_list_rcv(struct list_head *head, struct packet_type *pt,
 		struct net *net = dev_net(dev);
 
 		skb_list_del_init(skb);
-		skb = ip_rcv_core(skb, net);
+		skb = ip_rcv_core(skb, net, dev);
 		if (skb == NULL)
 			continue;
 
diff --git a/net/ipv4/ip_output.c b/net/ipv4/ip_output.c
index 97975bed491a..9e194f4e2050 100644
--- a/net/ipv4/ip_output.c
+++ b/net/ipv4/ip_output.c
@@ -82,6 +82,7 @@
 #include <linux/netfilter_bridge.h>
 #include <linux/netlink.h>
 #include <linux/tcp.h>
+#include <linux/sdt.h>
 
 static int
 ip_fragment(struct net *net, struct sock *sk, struct sk_buff *skb,
@@ -112,6 +113,14 @@ int __ip_local_out(struct net *net, struct sock *sk, struct sk_buff *skb)
 
 	skb->protocol = htons(ETH_P_IP);
 
+	DTRACE_IP(send,
+		  struct sk_buff * :  pktinfo_t *, skb,
+		  struct sock * : csinfo_t *, sk,
+		  void_ip_t * : ipinfo_t *, iph,
+		  struct net_device * : ifinfo_t *, skb->dev,
+		  struct iphdr * : ipv4info_t *, iph,
+		  struct ipv6hdr * : ipv6info_t *, NULL);
+
 	return nf_hook(NFPROTO_IPV4, NF_INET_LOCAL_OUT,
 		       net, sk, skb, NULL, skb_dst(skb)->dev,
 		       dst_output);
@@ -983,6 +992,7 @@ static int __ip_append_data(struct sock *sk,
 	unsigned int wmem_alloc_delta = 0;
 	bool paged, extra_uref = false;
 	u32 tskey = 0;
+	const char *dropreason;
 
 	skb = skb_peek_tail(queue);
 
@@ -1001,9 +1011,13 @@ static int __ip_append_data(struct sock *sk,
 	maxnonfragsize = ip_sk_ignore_df(sk) ? IP_MAX_MTU : mtu;
 
 	if (cork->length + length > maxnonfragsize - fragheaderlen) {
+		struct iphdr *iph __attribute__((unused)) = ip_hdr(skb);
+
 		ip_local_error(sk, EMSGSIZE, fl4->daddr, inet->inet_dport,
 			       mtu - (opt ? opt->optlen : 0));
-		return -EMSGSIZE;
+		dropreason = "packet too big";
+		err = -EMSGSIZE;
+		goto error2;
 	}
 
 	/*
@@ -1103,8 +1117,10 @@ static int __ip_append_data(struct sock *sk,
 				    2 * sk->sk_sndbuf)
 					skb = alloc_skb(alloclen + hh_len + 15,
 							sk->sk_allocation);
-				if (unlikely(!skb))
+				if (unlikely(!skb)) {
+					dropreason = "no buffers";
 					err = -ENOBUFS;
+				}
 			}
 			if (!skb)
 				goto error;
@@ -1138,7 +1154,9 @@ static int __ip_append_data(struct sock *sk,
 			copy = datalen - transhdrlen - fraggap - pagedlen;
 			if (copy > 0 && getfrag(from, data + transhdrlen, offset, copy, fraggap, skb) < 0) {
 				err = -EFAULT;
+				dropreason = "could not fragment packet";
 				kfree_skb(skb);
+				skb = NULL;
 				goto error;
 			}
 
@@ -1181,6 +1199,7 @@ static int __ip_append_data(struct sock *sk,
 			if (getfrag(from, skb_put(skb, copy),
 					offset, copy, off, skb) < 0) {
 				__skb_trim(skb, off);
+				dropreason = "could not fragment packet";
 				err = -EFAULT;
 				goto error;
 			}
@@ -1188,14 +1207,18 @@ static int __ip_append_data(struct sock *sk,
 			int i = skb_shinfo(skb)->nr_frags;
 
 			err = -ENOMEM;
-			if (!sk_page_frag_refill(sk, pfrag))
+			if (!sk_page_frag_refill(sk, pfrag)) {
+				dropreason = "no memory";
 				goto error;
+			}
 
 			if (!skb_can_coalesce(skb, i, pfrag->page,
 					      pfrag->offset)) {
 				err = -EMSGSIZE;
-				if (i == MAX_SKB_FRAGS)
+				if (i == MAX_SKB_FRAGS) {
+					dropreason = "too many fragments";
 					goto error;
+				}
 
 				__skb_fill_page_desc(skb, i, pfrag->page,
 						     pfrag->offset, 0);
@@ -1205,8 +1228,10 @@ static int __ip_append_data(struct sock *sk,
 			copy = min_t(int, copy, pfrag->size - pfrag->offset);
 			if (getfrag(from,
 				    page_address(pfrag->page) + pfrag->offset,
-				    offset, copy, skb->len, skb) < 0)
+				    offset, copy, skb->len, skb) < 0) {
+				dropreason = "could not framgent packet";
 				goto error_efault;
+			}
 
 			pfrag->offset += copy;
 			skb_frag_size_add(&skb_shinfo(skb)->frags[i - 1], copy);
@@ -1235,6 +1260,16 @@ static int __ip_append_data(struct sock *sk,
 	cork->length -= length;
 	IP_INC_STATS(sock_net(sk), IPSTATS_MIB_OUTDISCARDS);
 	refcount_add(wmem_alloc_delta, &sk->sk_wmem_alloc);
+error2:
+	DTRACE_IP(drop__out,
+		  struct sk_buff * : pktinfo_t *, skb,
+		  struct sock * : csinfo_t *, skb ? skb->sk : NULL,
+		  void_ip_t * : ipinfo_t *, skb ? ip_hdr(skb) : NULL,
+		  struct net_device * : ifinfo_t *, skb ? skb->dev : NULL,
+		  struct iphdr * : ipv4info_t *, skb ? ip_hdr(skb) : NULL,
+		  struct ipv6hdr * : ipv6info_t *, NULL,
+		  const char * : string, dropreason);
+
 	return err;
 }
 
@@ -1338,6 +1373,8 @@ ssize_t	ip_append_page(struct sock *sk, struct flowi4 *fl4, struct page *page,
 	int len;
 	int err;
 	unsigned int maxfraglen, fragheaderlen, fraggap, maxnonfragsize;
+	struct iphdr *iph;
+	const char *dropreason;
 
 	if (inet->hdrincl)
 		return -EPERM;
@@ -1391,6 +1428,7 @@ ssize_t	ip_append_page(struct sock *sk, struct flowi4 *fl4, struct page *page,
 			alloclen = fragheaderlen + hh_len + fraggap + 15;
 			skb = sock_wmalloc(sk, alloclen, 1, sk->sk_allocation);
 			if (unlikely(!skb)) {
+				dropreason = "no buffers";
 				err = -ENOBUFS;
 				goto error;
 			}
@@ -1430,6 +1468,7 @@ ssize_t	ip_append_page(struct sock *sk, struct flowi4 *fl4, struct page *page,
 			len = size;
 
 		if (skb_append_pagefrags(skb, page, offset, len)) {
+			dropreason = "packet too big";
 			err = -EMSGSIZE;
 			goto error;
 		}
@@ -1452,6 +1491,16 @@ ssize_t	ip_append_page(struct sock *sk, struct flowi4 *fl4, struct page *page,
 error:
 	cork->length -= size;
 	IP_INC_STATS(sock_net(sk), IPSTATS_MIB_OUTDISCARDS);
+	iph = skb ? ip_hdr(skb) : NULL;
+	DTRACE_IP(drop__out,
+		  struct sk_buff * : pktinfo_t *, skb,
+		  struct sock * : csinfo_t *, skb ? skb->sk : NULL,
+		  void_ip_t * : ipinfo_t *, iph,
+		  struct net_device * : ifinfo_t *, skb ? skb->dev : NULL,
+		  struct iphdr * : ipv4info_t *, iph,
+		  struct ipv6hdr * : ipv6info_t *, NULL,
+		  const char * : string, dropreason);
+
 	return err;
 }
 
@@ -1569,8 +1618,18 @@ int ip_send_skb(struct net *net, struct sk_buff *skb)
 	if (err) {
 		if (err > 0)
 			err = net_xmit_errno(err);
-		if (err)
+		if (err) {
 			IP_INC_STATS(net, IPSTATS_MIB_OUTDISCARDS);
+			/* skb may have been freed */
+			DTRACE_IP(drop__out,
+				  struct sk_buff * : pktinfo_t *, NULL,
+				  struct sock * : csinfo_t *, NULL,
+				  void_ip_t * : ipinfo_t *, NULL,
+				  struct net_device * : ifinfo_t *, NULL,
+				  struct iphdr * : ipv4info_t *, NULL,
+				  struct ipv6hdr * : ipv6info_t *, NULL,
+				  char * : string, "packet too short");
+		}
 	}
 
 	return err;
diff --git a/net/ipv4/raw.c b/net/ipv4/raw.c
index 7d26e0f8bdae..7ae1ad2bad09 100644
--- a/net/ipv4/raw.c
+++ b/net/ipv4/raw.c
@@ -75,6 +75,7 @@
 #include <linux/netfilter_ipv4.h>
 #include <linux/compat.h>
 #include <linux/uio.h>
+#include <linux/sdt.h>
 
 struct raw_frag_vec {
 	struct msghdr *msg;
@@ -349,19 +350,25 @@ static int raw_send_hdrinc(struct sock *sk, struct flowi4 *fl4,
 	struct inet_sock *inet = inet_sk(sk);
 	struct net *net = sock_net(sk);
 	struct iphdr *iph;
-	struct sk_buff *skb;
+	struct sk_buff *skb = NULL;
 	unsigned int iphlen;
 	int err;
 	struct rtable *rt = *rtp;
 	int hlen, tlen;
+	const char *dropreason;
 
 	if (length > rt->dst.dev->mtu) {
 		ip_local_error(sk, EMSGSIZE, fl4->daddr, inet->inet_dport,
 			       rt->dst.dev->mtu);
-		return -EMSGSIZE;
+		dropreason = "packet too big";
+		err = -EMSGSIZE;
+		goto trace_drop;
+	}
+	if (length < sizeof(struct iphdr)) {
+		dropreason = "packet too short";
+		err = -EINVAL;
+		goto trace_drop;
 	}
-	if (length < sizeof(struct iphdr))
-		return -EINVAL;
 
 	if (flags&MSG_PROBE)
 		goto out;
@@ -371,8 +378,10 @@ static int raw_send_hdrinc(struct sock *sk, struct flowi4 *fl4,
 	skb = sock_alloc_send_skb(sk,
 				  length + hlen + tlen + 15,
 				  flags & MSG_DONTWAIT, &err);
-	if (!skb)
+	if (!skb) {
+		dropreason = "out of memory";
 		goto error;
+	}
 	skb_reserve(skb, hlen);
 
 	skb->priority = sk->sk_priority;
@@ -394,8 +403,10 @@ static int raw_send_hdrinc(struct sock *sk, struct flowi4 *fl4,
 
 	skb->transport_header = skb->network_header;
 	err = -EFAULT;
-	if (memcpy_from_msg(iph, msg, length))
+	if (memcpy_from_msg(iph, msg, length)) {
+		dropreason = "could not copy msg";
 		goto error_free;
+	}
 
 	iphlen = iph->ihl * 4;
 
@@ -407,8 +418,10 @@ static int raw_send_hdrinc(struct sock *sk, struct flowi4 *fl4,
 	 * in, reject the frame as invalid
 	 */
 	err = -EINVAL;
-	if (iphlen > length)
+	if (iphlen > length) {
+		dropreason = "IP header too big";
 		goto error_free;
+	}
 
 	if (iphlen >= sizeof(*iph)) {
 		if (!iph->saddr)
@@ -426,20 +439,40 @@ static int raw_send_hdrinc(struct sock *sk, struct flowi4 *fl4,
 				skb_transport_header(skb))->type);
 	}
 
+	DTRACE_IP(send,
+		  struct sk_buff * :  pktinfo_t *, skb,
+		  struct sock * : csinfo_t *, sk,
+		  void_ip_t * : ipinfo_t *, iph,
+		  struct net_device * : ifinfo_t *, skb->dev,
+		  struct iphdr * : ipv4info_t *, iph,
+		  struct ipv6hdr * : ipv6info_t *, NULL);
+
 	err = NF_HOOK(NFPROTO_IPV4, NF_INET_LOCAL_OUT,
 		      net, sk, skb, NULL, rt->dst.dev,
 		      dst_output);
 	if (err > 0)
 		err = net_xmit_errno(err);
-	if (err)
+	if (err) {
+		dropreason = "device dropping packets of this priority";
 		goto error;
+	}
 out:
 	return 0;
 
 error_free:
 	kfree_skb(skb);
+	skb = NULL;
 error:
 	IP_INC_STATS(net, IPSTATS_MIB_OUTDISCARDS);
+trace_drop:
+	DTRACE_IP(drop__out,
+		  struct sk_buff * : pktinfo_t *, skb,
+		  struct sock * : csinfo_t *, skb ? skb->sk : NULL,
+		  void_ip_t * : ipinfo_t *, skb ? ip_hdr(skb) : NULL,
+		  struct net_device * : ifinfo_t *, skb ? skb->dev : NULL,
+		  struct iphdr * : ipv4info_t *, skb ? ip_hdr(skb) : NULL,
+		  struct ipv6hdr * : ipv6info_t *, NULL,
+		  const char * : string, dropreason);
 	if (err == -ENOBUFS && !inet->recverr)
 		err = 0;
 	return err;
diff --git a/net/ipv4/tcp.c b/net/ipv4/tcp.c
index 41d03683b13d..197946038626 100644
--- a/net/ipv4/tcp.c
+++ b/net/ipv4/tcp.c
@@ -267,6 +267,7 @@
 #include <linux/slab.h>
 #include <linux/errqueue.h>
 #include <linux/static_key.h>
+#include <linux/sdt.h>
 
 #include <net/icmp.h>
 #include <net/inet_common.h>
@@ -2277,6 +2278,19 @@ int tcp_recvmsg(struct sock *sk, struct msghdr *msg, size_t len, int nonblock,
 }
 EXPORT_SYMBOL(tcp_recvmsg);
 
+/* We wish to avoid instrumenting TCP state transitions to SYN_SENT as we trace
+ * those state changes later once the destination address is committed to the
+ * sk.  We also need to deal with the fact that separate timewait sockets are
+ * used to handle the TIME_WAIT state.  We do not want to trace direct
+ * transitions from CLOSING/FIN_WAIT2 -> CLOSE since they do not represent
+ * connection close, rather a transition to using the timewait socket.
+ * Accordingly skip instrumentation of transitions from CLOSING/FIN_WAIT2 to
+ * CLOSE.
+ */
+#define	REAL_STATE_CHANGE(old, new)					\
+	(old != new && new != TCP_SYN_SENT &&				\
+	((old != TCP_CLOSING && old != TCP_FIN_WAIT2) || new != TCP_CLOSE))
+
 void tcp_set_state(struct sock *sk, int state)
 {
 	int oldstate = sk->sk_state;
@@ -2329,6 +2343,18 @@ void tcp_set_state(struct sock *sk, int state)
 	 * socket sitting in hash tables.
 	 */
 	inet_sk_state_store(sk, state);
+
+	if (DTRACE_TCP_ENABLED(state__change) &&
+	    REAL_STATE_CHANGE(oldstate, state))
+		DTRACE_TCP_NOCHECK(state__change,
+				   struct sk_buff * : pktinfo_t *, NULL,
+				   struct sock * : csinfo_t *, sk,
+				   __dtrace_tcp_void_ip_t * : ipinfo_t *, NULL,
+				   struct tcp_sock * : tcpsinfo_t *, tcp_sk(sk),
+				   struct tcphdr * : tcpinfo_t *, NULL,
+				   int : tcplsinfo_t *, oldstate,
+				   int : int, state,
+				   int : int, DTRACE_NET_PROBE_OUTBOUND);
 }
 EXPORT_SYMBOL_GPL(tcp_set_state);
 
diff --git a/net/ipv4/tcp_input.c b/net/ipv4/tcp_input.c
index 6bf066f924c1..e29846f27328 100644
--- a/net/ipv4/tcp_input.c
+++ b/net/ipv4/tcp_input.c
@@ -80,6 +80,7 @@
 #include <linux/jump_label_ratelimit.h>
 #include <net/busy_poll.h>
 #include <net/mptcp.h>
+#include <linux/sdt.h>
 
 int sysctl_tcp_max_orphans __read_mostly = NR_FILE;
 
@@ -5914,6 +5915,14 @@ void tcp_finish_connect(struct sock *sk, struct sk_buff *skb)
 	struct tcp_sock *tp = tcp_sk(sk);
 	struct inet_connection_sock *icsk = inet_csk(sk);
 
+	DTRACE_TCP(connect__established,
+		   struct sk_buff * :  pktinfo_t *, skb,
+		   struct sock * : csinfo_t *, sk,
+		   __dtrace_tcp_void_ip_t * : ipinfo_t *, ip_hdr(skb),
+		   struct tcp_sock * : tcpsinfo_t *, tp,
+		   struct tcphdr * : tcpinfo_t *, tcp_hdr(skb),
+		   int : tcplsinfo_t *, TCP_ESTABLISHED,
+		   int, TCP_ESTABLISHED, int, DTRACE_NET_PROBE_INBOUND);
 	tcp_set_state(sk, TCP_ESTABLISHED);
 	icsk->icsk_ack.lrcvtime = tcp_jiffies32;
 
@@ -6078,6 +6087,17 @@ static int tcp_rcv_synsent_state_process(struct sock *sk, struct sk_buff *skb,
 		 */
 
 		if (th->rst) {
+			DTRACE_TCP(connect__refused,
+				   struct sk_buff * : pktinfo_t *, skb,
+				   struct sock * : csinfo_t *, sk,
+				   __dtrace_tcp_void_ip_t * : ipinfo_t *,
+				   ip_hdr(skb),
+				   struct tcp_sock * : tcpsinfo_t *, tp,
+				   struct tcphdr * : tcpinfo_t *, th,
+				   int : tcplsinfo_t *,
+				   sk ? sk->sk_state : TCP_CLOSE,
+				   int, sk ? sk->sk_state : TCP_CLOSE,
+				   int, DTRACE_NET_PROBE_INBOUND);
 			tcp_reset(sk);
 			goto discard;
 		}
@@ -6400,6 +6420,16 @@ int tcp_rcv_state_process(struct sock *sk, struct sk_buff *skb)
 			WRITE_ONCE(tp->copied_seq, tp->rcv_nxt);
 		}
 		smp_mb();
+
+		DTRACE_TCP(accept__established,
+			   struct sk_buff * :  pktinfo_t *, skb,
+			   struct sock * : csinfo_t *, sk,
+			   __dtrace_tcp_void_ip_t * : ipinfo_t *, ip_hdr(skb),
+			   struct tcp_sock * : tcpsinfo_t *, tp,
+			   struct tcphdr * : tcpinfo_t *, tcp_hdr(skb),
+			   int : tcplsinfo_t *, TCP_ESTABLISHED,
+			   int, TCP_ESTABLISHED,
+			   int, DTRACE_NET_PROBE_INBOUND);
 		tcp_set_state(sk, TCP_ESTABLISHED);
 		sk->sk_state_change(sk);
 
@@ -6873,6 +6903,19 @@ int tcp_conn_request(struct request_sock_ops *rsk_ops,
 				    !want_cookie ? TCP_SYNACK_NORMAL :
 						   TCP_SYNACK_COOKIE,
 				    skb);
+		/* Do not pass in tcp sock as ports/addresses are not yet
+		 * populated - instead translators will fill them in from
+		 * skb data.
+		 */
+		DTRACE_TCP(state__change,
+			   struct sk_buff * : pktinfo_t *, skb,
+			   struct sock * : csinfo_t *, sk,
+			   __dtrace_tcp_void_ip_t * : ipinfo_t *, ip_hdr(skb),
+			   struct tcp_sock * : tcpsinfo_t *, NULL,
+			   struct tcphdr * : tcpinfo_t *, tcp_hdr(skb),
+			   int : tcplsinfo_t *, TCP_LISTEN,
+			   int, TCP_SYN_RECV, int, DTRACE_NET_PROBE_INBOUND);
+
 		if (want_cookie) {
 			reqsk_free(req);
 			return 0;
diff --git a/net/ipv4/tcp_ipv4.c b/net/ipv4/tcp_ipv4.c
index ab8ed0fc4769..5f76469093a2 100644
--- a/net/ipv4/tcp_ipv4.c
+++ b/net/ipv4/tcp_ipv4.c
@@ -80,6 +80,7 @@
 
 #include <crypto/hash.h>
 #include <linux/scatterlist.h>
+#include <linux/sdt.h>
 
 #include <trace/events/tcp.h>
 
@@ -642,6 +643,21 @@ void tcp_v4_send_check(struct sock *sk, struct sk_buff *skb)
 }
 EXPORT_SYMBOL(tcp_v4_send_check);
 
+/* Since we want to trace send events in TCP prior to pushing the segment to
+ * IP - where the IP header is added - we need to construct an argument
+ * containing relevant IP info so that TCP probe consumers can utilize it.
+ */
+static inline void dtrace_tcp_build_iphdr(__be32 saddr, __be32 daddr,
+					  struct iphdr *iph)
+{
+	iph->version = 4;
+	iph->ihl = 5;
+	iph->tot_len = 5;
+	iph->protocol = IPPROTO_TCP;
+	iph->saddr = saddr;
+	iph->daddr = daddr;
+}
+
 /*
  *	This routine will send an RST to the other tcp.
  *
@@ -800,6 +816,39 @@ static void tcp_v4_send_reset(const struct sock *sk, struct sk_buff *skb)
 				   inet_twsk(sk)->tw_priority : sk->sk_priority;
 		transmit_time = tcp_transmit_time(sk);
 	}
+
+	if (DTRACE_TCP_ENABLED(send) ||
+	    DTRACE_TCP_ENABLED(accept__refused)) {
+		struct iphdr iph;
+
+		dtrace_tcp_build_iphdr(ip_hdr(skb)->daddr, ip_hdr(skb)->saddr,
+				       &iph);
+
+		DTRACE_TCP_NOCHECK(send,
+				   struct sk_buff * : pktinfo_t *, NULL,
+				   struct sock * : csinfo_t *, NULL,
+				   __dtrace_tcp_void_ip_t * : ipinfo_t *, &iph,
+				   struct tcp_sock * : tcpsinfo_t *, NULL,
+				   struct tcphdr * : tcpinfo_t *, &rep.th,
+				   int : tcplsinfo_t *, TCP_CLOSE,
+				   int : int, TCP_CLOSE,
+				   int : int, DTRACE_NET_PROBE_OUTBOUND);
+		if (th->syn && rep.th.seq == 0)
+			DTRACE_TCP_NOCHECK(accept__refused,
+					   struct sk_buff * : pktinfo_t *, NULL,
+					   struct sock * : csinfo_t *, NULL,
+					   __dtrace_tcp_void_ip_t * :
+					   ipinfo_t *, &iph,
+					   struct tcp_sock * : tcpsinfo_t *,
+					   NULL,
+					   struct tcphdr * : tcpinfo_t *,
+					   &rep.th,
+					   int : tcplsinfo_t *, TCP_CLOSE,
+					   int : int, TCP_CLOSE,
+					   int : int,
+					   DTRACE_NET_PROBE_OUTBOUND);
+	}
+
 	ip_send_unicast_reply(ctl_sk,
 			      skb, &TCP_SKB_CB(skb)->header.h4.opt,
 			      ip_hdr(skb)->saddr, ip_hdr(skb)->daddr,
@@ -896,6 +945,24 @@ static void tcp_v4_send_ack(const struct sock *sk,
 	ctl_sk->sk_priority = (sk->sk_state == TCP_TIME_WAIT) ?
 			   inet_twsk(sk)->tw_priority : sk->sk_priority;
 	transmit_time = tcp_transmit_time(sk);
+
+	if (DTRACE_TCP_ENABLED(send)) {
+		struct iphdr iph;
+
+		dtrace_tcp_build_iphdr(ip_hdr(skb)->daddr, ip_hdr(skb)->saddr,
+				       &iph);
+
+		DTRACE_TCP_NOCHECK(send,
+				   struct sk_buff * : pktinfo_t *, NULL,
+				   struct sock * : csinfo_t *, NULL,
+				   __dtrace_tcp_void_ip_t * : ipinfo_t *, &iph,
+				   struct tcp_sock * : tcpsinfo_t *, NULL,
+				   struct tcphdr * : tcpinfo_t *, &rep,
+				   int : tcplsinfo_t *, TCP_CLOSE,
+				   int : int, TCP_CLOSE,
+				   int : int, DTRACE_NET_PROBE_OUTBOUND);
+	}
+
 	ip_send_unicast_reply(ctl_sk,
 			      skb, &TCP_SKB_CB(skb)->header.h4.opt,
 			      ip_hdr(skb)->saddr, ip_hdr(skb)->daddr,
@@ -992,6 +1059,30 @@ static int tcp_v4_send_synack(const struct sock *sk, struct dst_entry *dst,
 		    tcp_bpf_ca_needs_ecn((struct sock *)req))
 			tos |= INET_ECN_ECT_0;
 
+		if (DTRACE_TCP_ENABLED(send)) {
+			struct iphdr iph;
+
+			dtrace_tcp_build_iphdr(ireq->ir_loc_addr,
+					       ireq->ir_rmt_addr, &iph);
+
+			/* Do not supply tcp sk - addresses/ports are not
+			 * committed yet - instead translators will fill them
+			 * in from skb/IP info.
+			 */
+			DTRACE_TCP_NOCHECK(send,
+					   struct sk_buff * :  pktinfo_t *, skb,
+					   struct sock * : csinfo_t *, sk,
+					   __dtrace_tcp_void_ip_t * :
+					   ipinfo_t *, &iph,
+					   struct tcp_sock * : tcpsinfo_t *,
+					   NULL,
+					   struct tcphdr * : tcpinfo_t *,
+					   tcp_hdr(skb),
+					   int : tcplsinfo_t *, TCP_LISTEN,
+					   int, TCP_LISTEN,
+					   int, DTRACE_NET_PROBE_OUTBOUND);
+		}
+
 		rcu_read_lock();
 		err = ip_build_and_send_pkt(skb, sk, ireq->ir_loc_addr,
 					    ireq->ir_rmt_addr,
@@ -1922,7 +2013,7 @@ int tcp_v4_rcv(struct sk_buff *skb)
 	const struct iphdr *iph;
 	const struct tcphdr *th;
 	bool refcounted;
-	struct sock *sk;
+	struct sock *sk = NULL;
 	int ret;
 
 	if (skb->pkt_type != PACKET_HOST)
@@ -1957,6 +2048,15 @@ int tcp_v4_rcv(struct sk_buff *skb)
 	if (!sk)
 		goto no_tcp_socket;
 
+	DTRACE_TCP(receive,
+		   struct sk_buff * :  pktinfo_t *, skb,
+		   struct sock * : csinfo_t *, sk,
+		   __dtrace_tcp_void_ip_t * : ipinfo_t *, ip_hdr(skb),
+		   struct tcp_sock * : tcpsinfo_t *, tcp_sk(sk),
+		   struct tcphdr * : tcpinfo_t *, tcp_hdr(skb),
+		   int : tcplsinfo_t *, sk ? sk->sk_state : TCP_CLOSE,
+		   int, sk ? sk->sk_state : TCP_CLOSE,
+		   int, DTRACE_NET_PROBE_INBOUND);
 process:
 	if (sk->sk_state == TCP_TIME_WAIT)
 		goto do_time_wait;
@@ -2084,6 +2184,18 @@ int tcp_v4_rcv(struct sk_buff *skb)
 
 discard_it:
 	/* Discard frame. */
+	if (DTRACE_TCP_ENABLED(receive) && skb->pkt_type == PACKET_HOST)
+		DTRACE_TCP_NOCHECK(receive,
+				   struct sk_buff * :  pktinfo_t *, skb,
+				   struct sock * : csinfo_t *, sk,
+				   __dtrace_tcp_void_ip_t * : ipinfo_t *,
+				   ip_hdr(skb),
+				   struct tcp_sock * : tcpsinfo_t *, tcp_sk(sk),
+				   struct tcphdr * : tcpinfo_t *, tcp_hdr(skb),
+				   int : tcplsinfo_t *,
+				   sk ? sk->sk_state : TCP_CLOSE,
+				   int, sk ? sk->sk_state : TCP_CLOSE,
+				   int, DTRACE_NET_PROBE_INBOUND);
 	kfree_skb(skb);
 	return 0;
 
diff --git a/net/ipv4/tcp_minisocks.c b/net/ipv4/tcp_minisocks.c
index 495dda2449fe..d2e368b7072d 100644
--- a/net/ipv4/tcp_minisocks.c
+++ b/net/ipv4/tcp_minisocks.c
@@ -24,6 +24,7 @@
 #include <linux/slab.h>
 #include <linux/sysctl.h>
 #include <linux/workqueue.h>
+#include <linux/sdt.h>
 #include <linux/static_key.h>
 #include <net/tcp.h>
 #include <net/inet_common.h>
@@ -328,6 +329,20 @@ void tcp_time_wait(struct sock *sk, int state, int timeo)
 		 */
 		inet_twsk_hashdance(tw, sk, &tcp_hashinfo);
 		local_bh_enable();
+
+		if (DTRACE_TCP_ENABLED(state__change) &&
+		    state != sk->sk_state)
+			DTRACE_TCP_NOCHECK(state__change,
+					   struct sk_buff * : pktinfo_t *, NULL,
+					   struct sock * : csinfo_t *, sk,
+					   __dtrace_tcp_void_ip_t * :
+					   ipinfo_t *, NULL,
+					   struct tcp_sock * : tcpsinfo_t *,
+					   tcp_sk(sk),
+					   struct tcphdr * : tcpinfo_t *, NULL,
+					   int : tcplsinfo_t *, sk->sk_state,
+					   int, state,
+					   int, DTRACE_NET_PROBE_OUTBOUND);
 	} else {
 		/* Sorry, if we're out of memory, just CLOSE this
 		 * socket up.  We've got bigger problems than
diff --git a/net/ipv4/tcp_output.c b/net/ipv4/tcp_output.c
index e58e2589d7f9..aee3d4d6a2ae 100644
--- a/net/ipv4/tcp_output.c
+++ b/net/ipv4/tcp_output.c
@@ -44,6 +44,7 @@
 #include <linux/gfp.h>
 #include <linux/module.h>
 #include <linux/static_key.h>
+#include <linux/sdt.h>
 
 #include <trace/events/tcp.h>
 
@@ -1384,6 +1385,27 @@ static int __tcp_transmit_skb(struct sock *sk, struct sk_buff *skb,
 		tp->bytes_sent += skb->len - tcp_header_size;
 	}
 
+	DTRACE_TCP(send,
+		   struct sk_buff * :  pktinfo_t *, skb,
+		   struct sock * : csinfo_t *, sk,
+		   __dtrace_tcp_void_ip_t * : ipinfo_t *, NULL,
+		   struct tcp_sock * : tcpsinfo_t *, tp,
+		   struct tcphdr * : tcpinfo_t *, tcp_hdr(skb),
+		   int : tcplsinfo_t *, sk->sk_state, int, sk->sk_state,
+		   int, DTRACE_NET_PROBE_OUTBOUND);
+	if (DTRACE_TCP_ENABLED(connect__request) && th->syn &&
+	    th->ack_seq == 0)
+		DTRACE_TCP_NOCHECK(connect__request,
+				   struct sk_buff * : pktinfo_t *, skb,
+				   struct sock * : csinfo_t *, sk,
+				   __dtrace_tcp_void_ip_t * : ipinfo_t *,
+				   ip_hdr(skb),
+				   struct tcp_sock * : tcpsinfo_t *, tp,
+				   struct tcphdr * : tcpinfo_t *, th,
+				   int : tcplsinfo_t *, sk->sk_state,
+				   int, sk->sk_state,
+				   int, DTRACE_NET_PROBE_OUTBOUND);
+
 	if (after(tcb->end_seq, tp->snd_nxt) || tcb->seq == tcb->end_seq)
 		TCP_ADD_STATS(sock_net(sk), TCP_MIB_OUTSEGS,
 			      tcp_skb_pcount(skb));
@@ -3845,6 +3867,13 @@ int tcp_connect(struct sock *sk)
 	tp->retrans_stamp = tcp_time_stamp(tp);
 	tcp_connect_queue_skb(sk, buff);
 	tcp_ecn_send_syn(sk, buff);
+	DTRACE_TCP(state__change, struct sk_buff * : pktinfo_t *, NULL,
+		   struct sock * : csinfo_t *, sk,
+		   __dtrace_tcp_void_ip_t * : ipinfo_t *, ip_hdr(buff),
+		   struct tcp_sock * : tcpsinfo_t *, tp,
+		   struct tcphdr * : tcpinfo_t *, tcp_hdr(buff),
+		   int : tcplsinfo_t *, TCP_CLOSE,
+		   int, sk->sk_state, int, DTRACE_NET_PROBE_OUTBOUND);
 	tcp_rbtree_insert(&sk->tcp_rtx_queue, buff);
 
 	/* Send off SYN; include data in Fast Open. */
diff --git a/net/ipv4/udp.c b/net/ipv4/udp.c
index e37a2fa65c29..5d32f5471d85 100644
--- a/net/ipv4/udp.c
+++ b/net/ipv4/udp.c
@@ -107,6 +107,7 @@
 #include <trace/events/udp.h>
 #include <linux/static_key.h>
 #include <linux/btf_ids.h>
+#include <linux/sdt.h>
 #include <trace/events/skb.h>
 #include <net/busy_poll.h>
 #include "udp_impl.h"
@@ -945,6 +946,13 @@ static int udp_send_skb(struct sk_buff *skb, struct flowi4 *fl4,
 		uh->check = CSUM_MANGLED_0;
 
 send:
+	DTRACE_UDP(send,
+		   struct sk_buff * :  pktinfo_t *, skb,
+		   struct sock * : csinfo_t *, sk,
+		   void_ip_t * : ipinfo_t *, ip_hdr(skb),
+		   struct udp_sock * : udpsinfo_t *, udp_sk(sk),
+		   struct udphdr * : udpinfo_t *, uh);
+
 	err = ip_send_skb(sock_net(sk), skb);
 	if (err) {
 		if (err == -ENOBUFS && !inet->recverr) {
@@ -1845,9 +1853,16 @@ int udp_recvmsg(struct sock *sk, struct msghdr *msg, size_t len, int noblock,
 		return err;
 	}
 
-	if (!peeking)
+	if (!peeking) {
+		DTRACE_UDP(receive,
+			   struct sk_buff * :  pktinfo_t *, skb,
+			   struct sock * : csinfo_t *, sk,
+			   void_ip_t * : ipinfo_t *, ip_hdr(skb),
+			   struct udp_sock * : udpsinfo_t *, udp_sk(sk),
+			   struct udphdr * : udpinfo_t *, udp_hdr(skb));
 		UDP_INC_STATS(sock_net(sk),
 			      UDP_MIB_INDATAGRAMS, is_udplite);
+	}
 
 	sock_recv_ts_and_drops(msg, sk, skb);
 
@@ -2092,6 +2107,15 @@ static int udp_queue_rcv_one_skb(struct sock *sk, struct sk_buff *skb)
 
 			ret = encap_rcv(sk, skb);
 			if (ret <= 0) {
+				DTRACE_UDP(receive,
+					   struct sk_buff * :  pktinfo_t *, skb,
+					   struct sock * : csinfo_t *, sk,
+					   void_ip_t * : ipinfo_t *,
+					   ip_hdr(skb),
+					   struct udp_sock * : udpsinfo_t *,
+					   udp_sk(sk),
+					   struct udphdr * : udpinfo_t *,
+					   udp_hdr(skb));
 				__UDP_INC_STATS(sock_net(sk),
 						UDP_MIB_INDATAGRAMS,
 						is_udplite);
diff --git a/net/ipv6/ip6_input.c b/net/ipv6/ip6_input.c
index e96304d8a4a7..aed39e366b60 100644
--- a/net/ipv6/ip6_input.c
+++ b/net/ipv6/ip6_input.c
@@ -43,6 +43,7 @@
 #include <net/xfrm.h>
 #include <net/inet_ecn.h>
 #include <net/dst_metadata.h>
+#include <linux/sdt.h>
 
 INDIRECT_CALLABLE_DECLARE(void udp_v6_early_demux(struct sk_buff *));
 INDIRECT_CALLABLE_DECLARE(void tcp_v6_early_demux(struct sk_buff *));
@@ -145,13 +146,14 @@ static void ip6_list_rcv_finish(struct net *net, struct sock *sk,
 static struct sk_buff *ip6_rcv_core(struct sk_buff *skb, struct net_device *dev,
 				    struct net *net)
 {
-	const struct ipv6hdr *hdr;
+	const struct ipv6hdr *hdr = NULL;
 	u32 pkt_len;
 	struct inet6_dev *idev;
+	const char *dropreason;
 
 	if (skb->pkt_type == PACKET_OTHERHOST) {
-		kfree_skb(skb);
-		return NULL;
+		dropreason = "for other host";
+		goto trace_drop;
 	}
 
 	rcu_read_lock();
@@ -163,6 +165,7 @@ static struct sk_buff *ip6_rcv_core(struct sk_buff *skb, struct net_device *dev,
 	if ((skb = skb_share_check(skb, GFP_ATOMIC)) == NULL ||
 	    !idev || unlikely(idev->cnf.disable_ipv6)) {
 		__IP6_INC_STATS(net, idev, IPSTATS_MIB_INDISCARDS);
+		dropreason = "could not clone shared buffer";
 		goto drop;
 	}
 
@@ -181,13 +184,18 @@ static struct sk_buff *ip6_rcv_core(struct sk_buff *skb, struct net_device *dev,
 	 */
 	IP6CB(skb)->iif = skb_valid_dst(skb) ? ip6_dst_idev(skb_dst(skb))->dev->ifindex : dev->ifindex;
 
-	if (unlikely(!pskb_may_pull(skb, sizeof(*hdr))))
+	if (unlikely(!pskb_may_pull(skb, sizeof(*hdr)))) {
+		hdr = ipv6_hdr(skb);
+		dropreason = "could not pull skb";
 		goto err;
+	}
 
 	hdr = ipv6_hdr(skb);
 
-	if (hdr->version != 6)
+	if (hdr->version != 6) {
+		dropreason = "header invalid";
 		goto err;
+	}
 
 	__IP6_ADD_STATS(net, idev,
 			IPSTATS_MIB_NOECTPKTS +
@@ -203,8 +211,10 @@ static struct sk_buff *ip6_rcv_core(struct sk_buff *skb, struct net_device *dev,
 	if ((ipv6_addr_loopback(&hdr->saddr) ||
 	     ipv6_addr_loopback(&hdr->daddr)) &&
 	    !(dev->flags & IFF_LOOPBACK) &&
-	    !netif_is_l3_master(dev))
+	    !netif_is_l3_master(dev)) {
+		dropreason = "loopback destination received on interface";
 		goto err;
+	}
 
 	/* RFC4291 Errata ID: 3480
 	 * Interface-Local scope spans only a single interface on a
@@ -215,8 +225,10 @@ static struct sk_buff *ip6_rcv_core(struct sk_buff *skb, struct net_device *dev,
 	if (!(skb->pkt_type == PACKET_LOOPBACK ||
 	      dev->flags & IFF_LOOPBACK) &&
 	    ipv6_addr_is_multicast(&hdr->daddr) &&
-	    IPV6_ADDR_MC_SCOPE(&hdr->daddr) == 1)
+	    IPV6_ADDR_MC_SCOPE(&hdr->daddr) == 1) {
+		dropreason = "interface-local scope received from other node";
 		goto err;
+	}
 
 	/* If enabled, drop unicast packets that were encapsulated in link-layer
 	 * multicast or broadcast to protected against the so-called "hole-196"
@@ -225,8 +237,10 @@ static struct sk_buff *ip6_rcv_core(struct sk_buff *skb, struct net_device *dev,
 	if (!ipv6_addr_is_multicast(&hdr->daddr) &&
 	    (skb->pkt_type == PACKET_BROADCAST ||
 	     skb->pkt_type == PACKET_MULTICAST) &&
-	    idev->cnf.drop_unicast_in_l2_multicast)
+	    idev->cnf.drop_unicast_in_l2_multicast) {
+		dropreason = "unicast packet encapsulated in multi/broadcast";
 		goto err;
+	}
 
 	/* RFC4291 2.7
 	 * Nodes must not originate a packet to a multicast address whose scope
@@ -234,16 +248,21 @@ static struct sk_buff *ip6_rcv_core(struct sk_buff *skb, struct net_device *dev,
 	 * must be silently dropped.
 	 */
 	if (ipv6_addr_is_multicast(&hdr->daddr) &&
-	    IPV6_ADDR_MC_SCOPE(&hdr->daddr) == 0)
+	    IPV6_ADDR_MC_SCOPE(&hdr->daddr) == 0) {
+		dropreason =
+		    "packet to multicast address with reserved scope 0";
 		goto err;
+	}
 
 	/*
 	 * RFC4291 2.7
 	 * Multicast addresses must not be used as source addresses in IPv6
 	 * packets or appear in any Routing header.
 	 */
-	if (ipv6_addr_is_multicast(&hdr->saddr))
+	if (ipv6_addr_is_multicast(&hdr->saddr)) {
+		dropreason = "multicast source address in IPv6 packet";
 		goto err;
+	}
 
 	/* While RFC4291 is not explicit about v4mapped addresses
 	 * in IPv6 headers, it seems clear linux dual-stack
@@ -253,7 +272,10 @@ static struct sk_buff *ip6_rcv_core(struct sk_buff *skb, struct net_device *dev,
 	 * https://tools.ietf.org/html/draft-itojun-v6ops-v4mapped-harmful-02
 	 */
 	if (ipv6_addr_v4mapped(&hdr->saddr))
+	{
+		dropreason = "v4-mapped address in IPv6 packet";
 		goto err;
+	}
 
 	skb->transport_header = skb->network_header + sizeof(*hdr);
 	IP6CB(skb)->nhoff = offsetof(struct ipv6hdr, nexthdr);
@@ -265,10 +287,12 @@ static struct sk_buff *ip6_rcv_core(struct sk_buff *skb, struct net_device *dev,
 		if (pkt_len + sizeof(struct ipv6hdr) > skb->len) {
 			__IP6_INC_STATS(net,
 					idev, IPSTATS_MIB_INTRUNCATEDPKTS);
+			dropreason = "truncated packet";
 			goto drop;
 		}
 		if (pskb_trim_rcsum(skb, pkt_len + sizeof(struct ipv6hdr))) {
 			__IP6_INC_STATS(net, idev, IPSTATS_MIB_INHDRERRORS);
+			dropreason = "could not trim buffer";
 			goto drop;
 		}
 		hdr = ipv6_hdr(skb);
@@ -276,9 +300,10 @@ static struct sk_buff *ip6_rcv_core(struct sk_buff *skb, struct net_device *dev,
 
 	if (hdr->nexthdr == NEXTHDR_HOP) {
 		if (ipv6_parse_hopopts(skb) < 0) {
-			__IP6_INC_STATS(net, idev, IPSTATS_MIB_INHDRERRORS);
-			rcu_read_unlock();
-			return NULL;
+			dropreason = "could not parse hop opts";
+			/* do not free skb */
+			skb = NULL;
+			goto err;
 		}
 	}
 
@@ -293,6 +318,15 @@ static struct sk_buff *ip6_rcv_core(struct sk_buff *skb, struct net_device *dev,
 	__IP6_INC_STATS(net, idev, IPSTATS_MIB_INHDRERRORS);
 drop:
 	rcu_read_unlock();
+trace_drop:
+	DTRACE_IP(drop__in,
+		  struct sk_buff * : pktinfo_t *, skb,
+		  struct sock * : csinfo_t *, skb ? skb->sk : NULL,
+		  void_ip_t * : ipinfo_t *, hdr,
+		  struct net_device * : ifinfo_t *, skb ? skb->dev : NULL,
+		  struct iphdr * : ipv4info_t *, NULL,
+		  struct ipv6hdr * : ipv6info_t *, hdr,
+		  const char * : string, dropreason);
 	kfree_skb(skb);
 	return NULL;
 }
@@ -365,6 +399,8 @@ void ip6_protocol_deliver_rcu(struct net *net, struct sk_buff *skb, int nexthdr,
 	struct inet6_dev *idev;
 	unsigned int nhoff;
 	bool raw;
+	const struct ipv6hdr *hdr;
+	const char *dropreason;
 
 	/*
 	 *	Parse extension headers
@@ -374,8 +410,10 @@ void ip6_protocol_deliver_rcu(struct net *net, struct sk_buff *skb, int nexthdr,
 	idev = ip6_dst_idev(skb_dst(skb));
 	nhoff = IP6CB(skb)->nhoff;
 	if (!have_final) {
-		if (!pskb_pull(skb, skb_transport_offset(skb)))
+		if (!pskb_pull(skb, skb_transport_offset(skb))) {
+			dropreason = "could not pull skb";
 			goto discard;
+		}
 		nexthdr = skb_network_header(skb)[nhoff];
 	}
 
@@ -392,10 +430,10 @@ void ip6_protocol_deliver_rcu(struct net *net, struct sk_buff *skb, int nexthdr,
 				 * ones. This allows foo in UDP encapsulation
 				 * to work.
 				 */
+				dropreason = "non-final protocol";
 				goto discard;
 			}
 		} else if (ipprot->flags & INET6_PROTO_FINAL) {
-			const struct ipv6hdr *hdr;
 			int sdif = inet6_sdif(skb);
 			struct net_device *dev;
 
@@ -414,8 +452,10 @@ void ip6_protocol_deliver_rcu(struct net *net, struct sk_buff *skb, int nexthdr,
 			/* skb->dev passed may be master dev for vrfs. */
 			if (sdif) {
 				dev = dev_get_by_index_rcu(net, sdif);
-				if (!dev)
+				if (!dev) {
+					dropreason = "device disappeared";
 					goto discard;
+				}
 			} else {
 				dev = skb->dev;
 			}
@@ -423,12 +463,16 @@ void ip6_protocol_deliver_rcu(struct net *net, struct sk_buff *skb, int nexthdr,
 			if (ipv6_addr_is_multicast(&hdr->daddr) &&
 			    !ipv6_chk_mcast_addr(dev, &hdr->daddr,
 						 &hdr->saddr) &&
-			    !ipv6_is_mld(skb, nexthdr, skb_network_header_len(skb)))
+			    !ipv6_is_mld(skb, nexthdr, skb_network_header_len(skb))) {
+				dropreason = "destination is multicast";
 				goto discard;
+			}
 		}
 		if (!(ipprot->flags & INET6_PROTO_NOPOLICY) &&
-		    !xfrm6_policy_check(NULL, XFRM_POLICY_IN, skb))
+		    !xfrm6_policy_check(NULL, XFRM_POLICY_IN, skb)) {
+			dropreason = "policy failure";
 			goto discard;
+		}
 
 		ret = INDIRECT_CALL_2(ipprot->handler, tcp_v6_rcv, udpv6_rcv,
 				      skb);
@@ -454,6 +498,8 @@ void ip6_protocol_deliver_rcu(struct net *net, struct sk_buff *skb, int nexthdr,
 						IPSTATS_MIB_INUNKNOWNPROTOS);
 				icmpv6_send(skb, ICMPV6_PARAMPROB,
 					    ICMPV6_UNK_NEXTHDR, nhoff);
+				dropreason = "policy failure";
+				goto trace_drop;
 			}
 			kfree_skb(skb);
 		} else {
@@ -465,6 +511,17 @@ void ip6_protocol_deliver_rcu(struct net *net, struct sk_buff *skb, int nexthdr,
 
 discard:
 	__IP6_INC_STATS(net, idev, IPSTATS_MIB_INDISCARDS);
+trace_drop:
+	hdr = ipv6_hdr(skb);
+	DTRACE_IP(drop__in,
+		  struct sk_buff * : pktinfo_t *, skb,
+		  struct sock * : csinfo_t *, skb->sk,
+		  void_ip_t * : ipinfo_t *, hdr,
+		  struct net_device * : ifinfo_t *, skb->dev,
+		  struct iphdr * : ipv4info_t *, NULL,
+		  struct ipv6hdr * : ipv6info_t *, hdr,
+		  const char * : string, dropreason);
+	rcu_read_unlock();
 	kfree_skb(skb);
 }
 
@@ -480,6 +537,16 @@ static int ip6_input_finish(struct net *net, struct sock *sk, struct sk_buff *sk
 
 int ip6_input(struct sk_buff *skb)
 {
+	struct ipv6hdr *hdr = ipv6_hdr(skb);
+
+	DTRACE_IP(receive,
+		  struct sk_buff * : pktinfo_t *, skb,
+		  struct sock * : csinfo_t *, skb->sk,
+		  void_ip_t * : ipinfo_t *, hdr,
+		  struct net_device * : ifinfo_t *, skb->dev,
+		  struct iphdr * : ipv4info_t *, NULL,
+		  struct ipv6hdr * : ipv6info_t *, hdr);
+
 	return NF_HOOK(NFPROTO_IPV6, NF_INET_LOCAL_IN,
 		       dev_net(skb->dev), NULL, skb, skb->dev, NULL,
 		       ip6_input_finish);
diff --git a/net/ipv6/ip6_output.c b/net/ipv6/ip6_output.c
index 077d43af8226..1c972fb379b5 100644
--- a/net/ipv6/ip6_output.c
+++ b/net/ipv6/ip6_output.c
@@ -55,6 +55,7 @@
 #include <net/l3mdev.h>
 #include <net/lwtunnel.h>
 #include <net/ip_tunnels.h>
+#include <linux/sdt.h>
 
 static int ip6_finish_output2(struct net *net, struct sock *sk, struct sk_buff *skb)
 {
@@ -62,7 +63,8 @@ static int ip6_finish_output2(struct net *net, struct sock *sk, struct sk_buff *
 	struct net_device *dev = dst->dev;
 	const struct in6_addr *nexthop;
 	struct neighbour *neigh;
-	int ret;
+	const char *dropreason;
+	int ret = 0;
 
 	if (ipv6_addr_is_multicast(&ipv6_hdr(skb)->daddr)) {
 		struct inet6_dev *idev = ip6_dst_idev(skb_dst(skb));
@@ -83,10 +85,11 @@ static int ip6_finish_output2(struct net *net, struct sock *sk, struct sk_buff *
 					dev_loopback_xmit);
 
 			if (ipv6_hdr(skb)->hop_limit == 0) {
+				dropreason = "hoplimit exceeded";
+
 				IP6_INC_STATS(net, idev,
 					      IPSTATS_MIB_OUTDISCARDS);
-				kfree_skb(skb);
-				return 0;
+				goto drop;
 			}
 		}
 
@@ -95,8 +98,8 @@ static int ip6_finish_output2(struct net *net, struct sock *sk, struct sk_buff *
 		if (IPV6_ADDR_MC_SCOPE(&ipv6_hdr(skb)->daddr) <=
 		    IPV6_ADDR_SCOPE_NODELOCAL &&
 		    !(dev->flags & IFF_LOOPBACK)) {
-			kfree_skb(skb);
-			return 0;
+			dropreason = "invalid scope";
+			goto drop;
 		}
 	}
 
@@ -120,9 +123,20 @@ static int ip6_finish_output2(struct net *net, struct sock *sk, struct sk_buff *
 	}
 	rcu_read_unlock_bh();
 
+	dropreason = "no route to host";
 	IP6_INC_STATS(net, ip6_dst_idev(dst), IPSTATS_MIB_OUTNOROUTES);
+	ret = -EINVAL;
+drop:
+	DTRACE_IP(drop__out,
+		  struct sk_buff * : pktinfo_t *, skb,
+		  struct sock * : csinfo_t *, skb->sk,
+		  void_ip_t * : ipinfo_t *, ipv6_hdr(skb),
+		  struct net_device * : ifinfo_t *, skb->dev,
+		  struct iphdr * : ipv4info_t *, NULL,
+		  struct ipv6hdr * : ipv6info_t *, ipv6_hdr(skb),
+		  const char * : string, dropreason);
 	kfree_skb(skb);
-	return -EINVAL;
+	return ret;
 }
 
 static int
@@ -207,6 +221,15 @@ int ip6_output(struct net *net, struct sock *sk, struct sk_buff *skb)
 	skb->dev = dev;
 
 	if (unlikely(idev->cnf.disable_ipv6)) {
+		DTRACE_IP(drop__out,
+			  struct sk_buff * : pktinfo_t *, skb,
+			  struct sock * : csinfo_t *, skb->sk,
+			  void_ip_t * : ipinfo_t *, NULL,
+			  struct net_device * : ifinfo_t *, skb->dev,
+			  struct iphdr * : ipv4info_t *, NULL,
+			  struct ipv6hdr * : ipv6info_t *, NULL,
+			  const char * : string, "IPv6 is disabled");
+
 		IP6_INC_STATS(net, idev, IPSTATS_MIB_OUTDISCARDS);
 		kfree_skb(skb);
 		return 0;
@@ -243,8 +266,10 @@ int ip6_xmit(const struct sock *sk, struct sk_buff *skb, struct flowi6 *fl6,
 	struct ipv6hdr *hdr;
 	u8  proto = fl6->flowi6_proto;
 	int seg_len = skb->len;
+	const char *dropreason;
 	int hlimit = -1;
 	u32 mtu;
+	int err;
 
 	head_room = sizeof(struct ipv6hdr) + LL_RESERVED_SPACE(dst->dev);
 	if (opt)
@@ -253,10 +278,12 @@ int ip6_xmit(const struct sock *sk, struct sk_buff *skb, struct flowi6 *fl6,
 	if (unlikely(skb_headroom(skb) < head_room)) {
 		struct sk_buff *skb2 = skb_realloc_headroom(skb, head_room);
 		if (!skb2) {
+			dropreason = "out of memory";
 			IP6_INC_STATS(net, ip6_dst_idev(skb_dst(skb)),
 				      IPSTATS_MIB_OUTDISCARDS);
 			kfree_skb(skb);
-			return -ENOBUFS;
+			err = -ENOBUFS;
+			goto drop;
 		}
 		if (skb->sk)
 			skb_set_owner_w(skb2, skb->sk);
@@ -313,6 +340,14 @@ int ip6_xmit(const struct sock *sk, struct sk_buff *skb, struct flowi6 *fl6,
 		if (unlikely(!skb))
 			return 0;
 
+		DTRACE_IP(send,
+			  struct sk_buff * : pktinfo_t *, skb,
+			  struct sock * : csinfo_t *, skb->sk,
+			  void_ip_t * : ipinfo_t *, hdr,
+			  struct net_device * : ifinfo_t *, skb->dev,
+			  struct iphdr * : ipv4info_t *, NULL,
+			  struct ipv6hdr * : ipv6info_t *, hdr);
+
 		/* hooks should never assume socket lock is held.
 		 * we promote our socket to non const
 		 */
@@ -327,9 +362,21 @@ int ip6_xmit(const struct sock *sk, struct sk_buff *skb, struct flowi6 *fl6,
 	 */
 	ipv6_local_error((struct sock *)sk, EMSGSIZE, fl6, mtu);
 
+	dropreason = "fragmentation failure";
 	IP6_INC_STATS(net, ip6_dst_idev(skb_dst(skb)), IPSTATS_MIB_FRAGFAILS);
+	err = -EMSGSIZE;
+drop:
+	DTRACE_IP(drop__out,
+		  struct sk_buff * : pktinfo_t *, skb,
+		  struct sock * : csinfo_t *, skb->sk,
+		  void_ip_t * : ipinfo_t *, NULL,
+		  struct net_device * : ifinfo_t *, skb->dev,
+		  struct iphdr * : ipv4info_t *, NULL,
+		  struct ipv6hdr * : ipv6info_t *, NULL,
+		  const char * : string, dropreason);
+
 	kfree_skb(skb);
-	return -EMSGSIZE;
+	return err;
 }
 EXPORT_SYMBOL(ip6_xmit);
 
@@ -464,22 +511,33 @@ int ip6_forward(struct sk_buff *skb)
 	struct ipv6hdr *hdr = ipv6_hdr(skb);
 	struct inet6_skb_parm *opt = IP6CB(skb);
 	struct net *net = dev_net(dst->dev);
+	const char *dropreason;
+	int err = -EINVAL;
 	u32 mtu;
 
-	if (net->ipv6.devconf_all->forwarding == 0)
+	if (net->ipv6.devconf_all->forwarding == 0) {
+		dropreason = "forwarding disabled";
 		goto error;
+	}
 
-	if (skb->pkt_type != PACKET_HOST)
+	if (skb->pkt_type != PACKET_HOST) {
+		dropreason = "non-host packet type cannot be forwarded";
 		goto drop;
+	}
 
-	if (unlikely(skb->sk))
+	if (unlikely(skb->sk)) {
+		dropreason = "socket found for packet to be forwarded";
 		goto drop;
+	}
 
-	if (skb_warn_if_lro(skb))
+	if (skb_warn_if_lro(skb)) {
+		dropreason = "LRO warning";
 		goto drop;
+	}
 
 	if (!xfrm6_policy_check(NULL, XFRM_POLICY_FWD, skb)) {
 		__IP6_INC_STATS(net, idev, IPSTATS_MIB_INDISCARDS);
+		dropreason = "forwarding disabled by policy";
 		goto drop;
 	}
 
@@ -510,8 +568,9 @@ int ip6_forward(struct sk_buff *skb)
 		icmpv6_send(skb, ICMPV6_TIME_EXCEED, ICMPV6_EXC_HOPLIMIT, 0);
 		__IP6_INC_STATS(net, idev, IPSTATS_MIB_INHDRERRORS);
 
-		kfree_skb(skb);
-		return -ETIMEDOUT;
+		dropreason = "hoplimit exceeded";
+		err = -ETIMEDOUT;
+		goto drop;
 	}
 
 	/* XXX: idev->cnf.proxy_ndp? */
@@ -521,6 +580,7 @@ int ip6_forward(struct sk_buff *skb)
 		if (proxied > 0)
 			return ip6_input(skb);
 		else if (proxied < 0) {
+			dropreason = "proxy router cannot forward";
 			__IP6_INC_STATS(net, idev, IPSTATS_MIB_INDISCARDS);
 			goto drop;
 		}
@@ -528,6 +588,7 @@ int ip6_forward(struct sk_buff *skb)
 
 	if (!xfrm6_route_forward(skb)) {
 		__IP6_INC_STATS(net, idev, IPSTATS_MIB_INDISCARDS);
+		dropreason = "forwarding disabled for destination";
 		goto drop;
 	}
 	dst = skb_dst(skb);
@@ -567,9 +628,12 @@ int ip6_forward(struct sk_buff *skb)
 
 		/* This check is security critical. */
 		if (addrtype == IPV6_ADDR_ANY ||
-		    addrtype & (IPV6_ADDR_MULTICAST | IPV6_ADDR_LOOPBACK))
+		    addrtype & (IPV6_ADDR_MULTICAST | IPV6_ADDR_LOOPBACK)) {
+			dropreason = "invalid address type for forwarding";
 			goto error;
+		}
 		if (addrtype & IPV6_ADDR_LINKLOCAL) {
+			dropreason = "invalid address type for forwarding";
 			icmpv6_send(skb, ICMPV6_DEST_UNREACH,
 				    ICMPV6_NOT_NEIGHBOUR, 0);
 			goto error;
@@ -583,17 +647,20 @@ int ip6_forward(struct sk_buff *skb)
 	if (ip6_pkt_too_big(skb, mtu)) {
 		/* Again, force OUTPUT device used as source address */
 		skb->dev = dst->dev;
+
 		icmpv6_send(skb, ICMPV6_PKT_TOOBIG, 0, mtu);
 		__IP6_INC_STATS(net, idev, IPSTATS_MIB_INTOOBIGERRORS);
 		__IP6_INC_STATS(net, ip6_dst_idev(dst),
 				IPSTATS_MIB_FRAGFAILS);
-		kfree_skb(skb);
-		return -EMSGSIZE;
+		dropreason = "packet too big";
+		err = -EMSGSIZE;
+		goto drop;
 	}
 
 	if (skb_cow(skb, dst->dev->hard_header_len)) {
 		__IP6_INC_STATS(net, ip6_dst_idev(dst),
 				IPSTATS_MIB_OUTDISCARDS);
+		dropreason = "copy-on-write failed";
 		goto drop;
 	}
 
@@ -610,6 +677,15 @@ int ip6_forward(struct sk_buff *skb)
 error:
 	__IP6_INC_STATS(net, idev, IPSTATS_MIB_INADDRERRORS);
 drop:
+	DTRACE_IP(drop__out,
+		  struct sk_buff * : pktinfo_t *, skb,
+		  struct sock * : csinfo_t *, skb->sk,
+		  void_ip_t * : ipinfo_t *, hdr,
+		  struct net_device * : ifinfo_t *, skb->dev,
+		  struct iphdr * : ipv4info_t *, NULL,
+		  struct ipv6hdr * : ipv6info_t *, hdr,
+		  const char * : string, dropreason);
+
 	kfree_skb(skb);
 	return -EINVAL;
 }
@@ -1445,6 +1521,7 @@ static int __ip6_append_data(struct sock *sk,
 	unsigned int maxnonfragsize, headersize;
 	unsigned int wmem_alloc_delta = 0;
 	bool paged, extra_uref = false;
+	const char *dropreason;
 
 	skb = skb_peek_tail(queue);
 	if (!skb) {
@@ -1484,6 +1561,7 @@ static int __ip6_append_data(struct sock *sk,
 	     sk->sk_protocol == IPPROTO_RAW)) {
 		ipv6_local_rxpmtu(sk, fl6, mtu - headersize +
 				sizeof(struct ipv6hdr));
+		dropreason = "fragmentation needed but disabled";
 		goto emsgsize;
 	}
 
@@ -1496,7 +1574,9 @@ static int __ip6_append_data(struct sock *sk,
 emsgsize:
 		pmtu = max_t(int, mtu - headersize + sizeof(struct ipv6hdr), 0);
 		ipv6_local_error(sk, EMSGSIZE, fl6, pmtu);
-		return -EMSGSIZE;
+		dropreason = "packet too big";
+		err = -EMSGSIZE;
+		goto trace_drop;
 	}
 
 	/* CHECKSUM_PARTIAL only with no extension headers and when
@@ -1511,8 +1591,11 @@ static int __ip6_append_data(struct sock *sk,
 
 	if (flags & MSG_ZEROCOPY && length && sock_flag(sk, SOCK_ZEROCOPY)) {
 		uarg = sock_zerocopy_realloc(sk, length, skb_zcopy(skb));
-		if (!uarg)
-			return -ENOBUFS;
+		if (!uarg) {
+			err = -ENOBUFS;
+			dropreason = "out of memory";
+			goto error;
+		}
 		extra_uref = !skb_zcopy(skb);	/* only ref on new uarg */
 		if (rt->dst.dev->features & NETIF_F_SG &&
 		    csummode == CHECKSUM_PARTIAL) {
@@ -1614,6 +1697,7 @@ static int __ip6_append_data(struct sock *sk,
 			copy = datalen - transhdrlen - fraggap - pagedlen;
 			if (copy < 0) {
 				err = -EINVAL;
+				dropreason = "invalid fragment";
 				goto error;
 			}
 			if (transhdrlen) {
@@ -1626,11 +1710,13 @@ static int __ip6_append_data(struct sock *sk,
 				    2 * sk->sk_sndbuf)
 					skb = alloc_skb(alloclen + hh_len,
 							sk->sk_allocation);
-				if (unlikely(!skb))
-					err = -ENOBUFS;
 			}
-			if (!skb)
+			if (unlikely(!skb)) {
+				err = -ENOBUFS;
+				dropreason = "out of memory";
 				goto error;
+			}
+
 			/*
 			 *	Fill in the control structures
 			 */
@@ -1662,7 +1748,9 @@ static int __ip6_append_data(struct sock *sk,
 			    getfrag(from, data + transhdrlen, offset,
 				    copy, fraggap, skb) < 0) {
 				err = -EFAULT;
+				dropreason = "could not get fragment";
 				kfree_skb(skb);
+				skb = NULL;
 				goto error;
 			}
 
@@ -1706,20 +1794,25 @@ static int __ip6_append_data(struct sock *sk,
 						offset, copy, off, skb) < 0) {
 				__skb_trim(skb, off);
 				err = -EFAULT;
+				dropreason = "could not get fragment";
 				goto error;
 			}
 		} else if (!uarg || !uarg->zerocopy) {
 			int i = skb_shinfo(skb)->nr_frags;
 
 			err = -ENOMEM;
-			if (!sk_page_frag_refill(sk, pfrag))
+			if (!sk_page_frag_refill(sk, pfrag)) {
+				dropreason = "out of memory";
 				goto error;
+			}
 
 			if (!skb_can_coalesce(skb, i, pfrag->page,
 					      pfrag->offset)) {
 				err = -EMSGSIZE;
-				if (i == MAX_SKB_FRAGS)
+				if (i == MAX_SKB_FRAGS) {
+					dropreason = "too many fragments";
 					goto error;
+				}
 
 				__skb_fill_page_desc(skb, i, pfrag->page,
 						     pfrag->offset, 0);
@@ -1729,8 +1822,10 @@ static int __ip6_append_data(struct sock *sk,
 			copy = min_t(int, copy, pfrag->size - pfrag->offset);
 			if (getfrag(from,
 				    page_address(pfrag->page) + pfrag->offset,
-				    offset, copy, skb->len, skb) < 0)
+				    offset, copy, skb->len, skb) < 0) {
+				dropreason = "could not get fragment";
 				goto error_efault;
+			}
 
 			pfrag->offset += copy;
 			skb_frag_size_add(&skb_shinfo(skb)->frags[i - 1], copy);
@@ -1740,8 +1835,10 @@ static int __ip6_append_data(struct sock *sk,
 			wmem_alloc_delta += copy;
 		} else {
 			err = skb_zerocopy_iter_dgram(skb, from, copy);
-			if (err < 0)
+			if (err < 0) {
+				dropreason = "skb iteration failure\n";
 				goto error;
+			}
 		}
 		offset += copy;
 		length -= copy;
@@ -1758,6 +1855,16 @@ static int __ip6_append_data(struct sock *sk,
 		sock_zerocopy_put_abort(uarg, extra_uref);
 	cork->length -= length;
 	IP6_INC_STATS(sock_net(sk), rt->rt6i_idev, IPSTATS_MIB_OUTDISCARDS);
+trace_drop:
+	DTRACE_IP(drop__out,
+		  struct sk_buff * : pktinfo_t *, skb,
+		  struct sock * : csinfo_t *, skb ? skb->sk : NULL,
+		  void_ip_t * : ipinfo_t *, NULL,
+		  struct net_device * : ifinfo_t *, skb ? skb->dev : NULL,
+		  struct iphdr * : ipv4info_t *, NULL,
+		  struct ipv6hdr * : ipv6info_t *, NULL,
+		  const char * : string, dropreason);
+
 	refcount_add(wmem_alloc_delta, &sk->sk_wmem_alloc);
 	return err;
 }
@@ -1905,9 +2012,20 @@ int ip6_send_skb(struct sk_buff *skb)
 	if (err) {
 		if (err > 0)
 			err = net_xmit_errno(err);
-		if (err)
+		if (err) {
+			/* skb may have been freed */
+			DTRACE_IP(drop__out,
+				  struct sk_buff * : pktinfo_t *, NULL,
+				  struct sock * : csinfo_t *, NULL,
+				  void_ip_t * : ipinfo_t *, NULL,
+				  struct net_device * : ifinfo_t *, NULL,
+				  struct iphdr * : ipv4info_t *, NULL,
+				  struct ipv6hdr * : ipv6info_t *, NULL,
+				  const char * : string, "out of memory");
+
 			IP6_INC_STATS(net, rt->rt6i_idev,
 				      IPSTATS_MIB_OUTDISCARDS);
+		}
 	}
 
 	return err;
@@ -1933,9 +2051,19 @@ static void __ip6_flush_pending_frames(struct sock *sk,
 	struct sk_buff *skb;
 
 	while ((skb = __skb_dequeue_tail(queue)) != NULL) {
-		if (skb_dst(skb))
+		if (skb_dst(skb)) {
+			DTRACE_IP(drop__out,
+				  struct sk_buff * : pktinfo_t *, skb,
+				  struct sock * : csinfo_t *, skb->sk,
+				  void_ip_t * : ipinfo_t *, ipv6_hdr(skb),
+				  struct net_device * : ifinfo_t *, skb->dev,
+				  struct iphdr * : ipv4info_t *, NULL,
+				  struct ipv6hdr * : ipv6info_t *,
+				  ipv6_hdr(skb),
+				  const char * : string, "flushing pending frames");
 			IP6_INC_STATS(sock_net(sk), ip6_dst_idev(skb_dst(skb)),
 				      IPSTATS_MIB_OUTDISCARDS);
+		}
 		kfree_skb(skb);
 	}
 
diff --git a/net/ipv6/mcast.c b/net/ipv6/mcast.c
index 8cd2782a31e4..e90cb6e0781d 100644
--- a/net/ipv6/mcast.c
+++ b/net/ipv6/mcast.c
@@ -60,6 +60,8 @@
 
 #include <net/ip6_checksum.h>
 
+#include <linux/sdt.h>
+
 /* Ensure that we have struct in6_addr aligned on 32bit word. */
 static int __mld2_query_bugs[] __attribute__((__unused__)) = {
 	BUILD_BUG_ON_ZERO(offsetof(struct mld2_query, mld2q_srcs) % 4),
@@ -1644,6 +1646,7 @@ static void mld_sendpack(struct sk_buff *skb)
 	int payload_len, mldlen;
 	struct inet6_dev *idev;
 	struct net *net = dev_net(skb->dev);
+	const char *dropreason;
 	int err;
 	struct flowi6 fl6;
 	struct dst_entry *dst;
@@ -1673,26 +1676,45 @@ static void mld_sendpack(struct sk_buff *skb)
 		dst = NULL;
 	}
 	skb_dst_set(skb, dst);
-	if (err)
-		goto err_out;
+	if (err) {
+		kfree_skb(skb);
+		skb = NULL;
+		dropreason = "out of memory";
+		goto out;
+	}
+
+	DTRACE_IP(send,
+		  struct sk_buff * : pktinfo_t *, skb,
+		  struct sock * : csinfo_t *, skb->sk,
+		  void_ip_t * : ipinfo_t *, ipv6_hdr(skb),
+		  struct net_device * : ifinfo_t *, skb->dev,
+		  struct iphdr * : ipv4info_t *, NULL,
+		  struct ipv6hdr * : ipv6info_t *, ipv6_hdr(skb));
 
 	err = NF_HOOK(NFPROTO_IPV6, NF_INET_LOCAL_OUT,
 		      net, net->ipv6.igmp_sk, skb, NULL, skb->dev,
 		      dst_output);
+	dropreason = "multicast send error";
 out:
 	if (!err) {
 		ICMP6MSGOUT_INC_STATS(net, idev, ICMPV6_MLD2_REPORT);
 		ICMP6_INC_STATS(net, idev, ICMP6_MIB_OUTMSGS);
 	} else {
+		/* skb may have been freed */
+		DTRACE_IP(drop__out,
+			  struct sk_buff * : pktinfo_t *, NULL,
+			  struct sock * : csinfo_t *, NULL,
+			  void_ip_t * : ipinfo_t *, NULL,
+			  struct net_device * : ifinfo_t *, idev->dev,
+			  struct iphdr * : ipv4info_t *, NULL,
+			  struct ipv6hdr * : ipv6info_t *, NULL,
+			  const char * : string, dropreason);
+
 		IP6_INC_STATS(net, idev, IPSTATS_MIB_OUTDISCARDS);
 	}
 
 	rcu_read_unlock();
 	return;
-
-err_out:
-	kfree_skb(skb);
-	goto out;
 }
 
 static int grec_size(struct ifmcaddr6 *pmc, int type, int gdel, int sdel)
@@ -1979,7 +2001,7 @@ static void igmp6_send(struct in6_addr *addr, struct net_device *dev, int type)
 {
 	struct net *net = dev_net(dev);
 	struct sock *sk = net->ipv6.igmp_sk;
-	struct inet6_dev *idev;
+	struct inet6_dev *idev = NULL;
 	struct sk_buff *skb;
 	struct mld_msg *hdr;
 	const struct in6_addr *snd_addr, *saddr;
@@ -1990,6 +2012,7 @@ static void igmp6_send(struct in6_addr *addr, struct net_device *dev, int type)
 	u8 ra[8] = { IPPROTO_ICMPV6, 0,
 		     IPV6_TLV_ROUTERALERT, 2, 0, 0,
 		     IPV6_TLV_PADN, 0 };
+	const char *dropreason;
 	struct flowi6 fl6;
 	struct dst_entry *dst;
 
@@ -2011,10 +2034,8 @@ static void igmp6_send(struct in6_addr *addr, struct net_device *dev, int type)
 
 	if (!skb) {
 		rcu_read_lock();
-		IP6_INC_STATS(net, __in6_dev_get(dev),
-			      IPSTATS_MIB_OUTDISCARDS);
-		rcu_read_unlock();
-		return;
+		dropreason = "out of memory";
+		goto out;
 	}
 	skb->priority = TC_PRIO_CONTROL;
 	skb_reserve(skb, hlen);
@@ -2049,26 +2070,43 @@ static void igmp6_send(struct in6_addr *addr, struct net_device *dev, int type)
 	dst = icmp6_dst_alloc(skb->dev, &fl6);
 	if (IS_ERR(dst)) {
 		err = PTR_ERR(dst);
-		goto err_out;
+		kfree_skb(skb);
+		skb = NULL;
+		dropreason = "out of memory";
+		goto out;
 	}
 
 	skb_dst_set(skb, dst);
+	DTRACE_IP(send,
+		  struct sk_buff * : pktinfo_t *, skb,
+		  struct sock * : csinfo_t *, skb->sk,
+		  void_ip_t * : ipinfo_t *, ipv6_hdr(skb),
+		  struct net_device * : ifinfo_t *, skb->dev,
+		  struct iphdr * : ipv4info_t *, NULL,
+		  struct ipv6hdr * : ipv6info_t *, ipv6_hdr(skb));
 	err = NF_HOOK(NFPROTO_IPV6, NF_INET_LOCAL_OUT,
 		      net, sk, skb, NULL, skb->dev,
 		      dst_output);
+	dropreason = "multicast send error";
 out:
 	if (!err) {
 		ICMP6MSGOUT_INC_STATS(net, idev, type);
 		ICMP6_INC_STATS(net, idev, ICMP6_MIB_OUTMSGS);
-	} else
+	} else {
+		/* skb may have been freed */
+		DTRACE_IP(drop__out,
+			  struct sk_buff * : pktinfo_t *, NULL,
+			  struct sock * : csinfo_t *, sk,
+			  void_ip_t * : ipinfo_t *, NULL,
+			  struct net_device * : ifinfo_t *, idev->dev,
+			  struct iphdr * : ipv4info_t *, NULL,
+			  struct ipv6hdr * : ipv6info_t *, NULL,
+			  const char * : string, dropreason);
 		IP6_INC_STATS(net, idev, IPSTATS_MIB_OUTDISCARDS);
+	}
 
 	rcu_read_unlock();
 	return;
-
-err_out:
-	kfree_skb(skb);
-	goto out;
 }
 
 static void mld_send_initial_cr(struct inet6_dev *idev)
diff --git a/net/ipv6/ndisc.c b/net/ipv6/ndisc.c
index 76717478f173..35ae4eaffd89 100644
--- a/net/ipv6/ndisc.c
+++ b/net/ipv6/ndisc.c
@@ -68,6 +68,7 @@
 
 #include <linux/netfilter.h>
 #include <linux/netfilter_ipv6.h>
+#include <linux/sdt.h>
 
 static u32 ndisc_hash(const void *pkey,
 		      const struct net_device *dev,
@@ -505,6 +506,14 @@ static void ndisc_send_skb(struct sk_buff *skb,
 	idev = __in6_dev_get(dst->dev);
 	IP6_UPD_PO_STATS(net, idev, IPSTATS_MIB_OUT, skb->len);
 
+	DTRACE_IP(send,
+		  struct sk_buff * : pktinfo_t *, skb,
+		  struct sock * : csinfo_t *, skb->sk,
+		  void_ip_t * : ipinfo_t *, ipv6_hdr(skb),
+		  struct net_device * : ifinfo_t *, skb->dev,
+		  struct iphdr * : ipv4info_t *, NULL,
+		  struct ipv6hdr * : ipv6info_t *, ipv6_hdr(skb));
+
 	err = NF_HOOK(NFPROTO_IPV6, NF_INET_LOCAL_OUT,
 		      net, sk, skb, NULL, dst->dev,
 		      dst_output);
diff --git a/net/ipv6/output_core.c b/net/ipv6/output_core.c
index af36acc1a644..811a88767a5c 100644
--- a/net/ipv6/output_core.c
+++ b/net/ipv6/output_core.c
@@ -10,6 +10,7 @@
 #include <net/addrconf.h>
 #include <net/secure_seq.h>
 #include <linux/netfilter.h>
+#include <linux/sdt.h>
 
 static u32 __ipv6_select_ident(struct net *net,
 			       const struct in6_addr *dst,
@@ -164,6 +165,14 @@ int __ip6_local_out(struct net *net, struct sock *sk, struct sk_buff *skb)
 
 	skb->protocol = htons(ETH_P_IPV6);
 
+	DTRACE_IP(send,
+		  struct sk_buff * : pktinfo_t *, skb,
+		  struct sock * : csinfo_t *, skb->sk,
+		  void_ip_t * : ipinfo_t *, ipv6_hdr(skb),
+		  struct net_device * : ifinfo_t *, skb->dev,
+		  struct iphdr * : ipv4info_t *, NULL,
+		  struct ipv6hdr * : ipv6info_t *, ipv6_hdr(skb));
+
 	return nf_hook(NFPROTO_IPV6, NF_INET_LOCAL_OUT,
 		       net, sk, skb, NULL, skb_dst(skb)->dev,
 		       dst_output);
diff --git a/net/ipv6/raw.c b/net/ipv6/raw.c
index 6e4ab80a3b94..f88c9441ec51 100644
--- a/net/ipv6/raw.c
+++ b/net/ipv6/raw.c
@@ -58,6 +58,7 @@
 #include <linux/proc_fs.h>
 #include <linux/seq_file.h>
 #include <linux/export.h>
+#include <linux/sdt.h>
 
 #define	ICMPV6_HDRLEN	4	/* ICMPv6 header, RFC 4443 Section 2.1 */
 
@@ -622,26 +623,34 @@ static int rawv6_send_hdrinc(struct sock *sk, struct msghdr *msg, int length,
 	struct ipv6_pinfo *np = inet6_sk(sk);
 	struct net *net = sock_net(sk);
 	struct ipv6hdr *iph;
-	struct sk_buff *skb;
+	struct sk_buff *skb = NULL;
 	int err;
 	struct rt6_info *rt = (struct rt6_info *)*dstp;
 	int hlen = LL_RESERVED_SPACE(rt->dst.dev);
 	int tlen = rt->dst.dev->needed_tailroom;
+	const char *dropreason;
 
 	if (length > rt->dst.dev->mtu) {
 		ipv6_local_error(sk, EMSGSIZE, fl6, rt->dst.dev->mtu);
-		return -EMSGSIZE;
+		dropreason = "packet too big";
+		err = -EMSGSIZE;
+		goto error_check;
+	}
+	if (length < sizeof(struct ipv6hdr)) {
+		dropreason = "packet too short";
+		err = -EINVAL;
+		goto error_check;
 	}
-	if (length < sizeof(struct ipv6hdr))
-		return -EINVAL;
 	if (flags&MSG_PROBE)
 		goto out;
 
 	skb = sock_alloc_send_skb(sk,
 				  length + hlen + tlen + 15,
 				  flags & MSG_DONTWAIT, &err);
-	if (!skb)
+	if (!skb) {
+		dropreason = "out of memory";
 		goto error;
+	}
 	skb_reserve(skb, hlen);
 
 	skb->protocol = htons(ETH_P_IPV6);
@@ -665,7 +674,8 @@ static int rawv6_send_hdrinc(struct sock *sk, struct msghdr *msg, int length,
 	if (err) {
 		err = -EFAULT;
 		kfree_skb(skb);
-		goto error;
+		dropreason = "could not copy msg";
+		goto error_check;
 	}
 
 	skb_dst_set(skb, &rt->dst);
@@ -684,6 +694,13 @@ static int rawv6_send_hdrinc(struct sock *sk, struct msghdr *msg, int length,
 	 */
 	rcu_read_lock();
 	IP6_UPD_PO_STATS(net, rt->rt6i_idev, IPSTATS_MIB_OUT, skb->len);
+	DTRACE_IP(send,
+		  struct sk_buff * : pktinfo_t *, skb,
+		  struct sock * : csinfo_t *, skb->sk,
+		  void_ip_t * : ipinfo_t *, ipv6_hdr(skb),
+		  struct net_device * : ifinfo_t *, skb->dev,
+		  struct iphdr * : ipv4info_t *, NULL,
+		  struct ipv6hdr * : ipv6info_t *, ipv6_hdr(skb));
 	err = NF_HOOK(NFPROTO_IPV6, NF_INET_LOCAL_OUT, net, sk, skb,
 		      NULL, rt->dst.dev, dst_output);
 	if (err > 0)
@@ -691,6 +708,7 @@ static int rawv6_send_hdrinc(struct sock *sk, struct msghdr *msg, int length,
 	if (err) {
 		IP6_INC_STATS(net, rt->rt6i_idev, IPSTATS_MIB_OUTDISCARDS);
 		rcu_read_unlock();
+		dropreason = "raw send error";
 		goto error_check;
 	}
 	rcu_read_unlock();
@@ -700,6 +718,14 @@ static int rawv6_send_hdrinc(struct sock *sk, struct msghdr *msg, int length,
 error:
 	IP6_INC_STATS(net, rt->rt6i_idev, IPSTATS_MIB_OUTDISCARDS);
 error_check:
+	DTRACE_IP(drop__out,
+		  struct sk_buff * : pktinfo_t *, skb,
+		  struct sock * : csinfo_t *, skb ? skb->sk : NULL,
+		  void_ip_t * : ipinfo_t *, skb ? ipv6_hdr(skb) : NULL,
+		  struct net_device * : ifinfo_t *, skb ? skb->dev : NULL,
+		  struct iphdr * : ipv4info_t *, NULL,
+		  struct ipv6hdr * : ipv6info_t *, skb ? ipv6_hdr(skb) : NULL,
+		  const char * : string, dropreason);
 	if (err == -ENOBUFS && !np->recverr)
 		err = 0;
 	return err;
diff --git a/net/ipv6/tcp_ipv6.c b/net/ipv6/tcp_ipv6.c
index 991dc36f95ff..acc63c455651 100644
--- a/net/ipv6/tcp_ipv6.c
+++ b/net/ipv6/tcp_ipv6.c
@@ -65,6 +65,7 @@
 
 #include <crypto/hash.h>
 #include <linux/scatterlist.h>
+#include <linux/sdt.h>
 
 #include <trace/events/tcp.h>
 
@@ -496,6 +497,20 @@ static int tcp_v6_err(struct sk_buff *skb, struct inet6_skb_parm *opt,
 	return 0;
 }
 
+/* Since we want to trace send events in TCP prior to pushing the segment to
+ * IP - where the IP header is added - we need to construct an argument
+ * containing relevant IP info so that TCP probe consumers can utilize it.
+ */
+static inline void dtrace_tcp_build_ipv6hdr(struct in6_addr *saddr,
+					    struct in6_addr *daddr,
+					    struct ipv6hdr *ip6h)
+{
+	ip6h->version = 6;
+	ip6h->payload_len = 0;
+	ip6h->nexthdr = IPPROTO_TCP;
+	ip6h->saddr = *saddr;
+	ip6h->daddr = *daddr;
+}
 
 static int tcp_v6_send_synack(const struct sock *sk, struct dst_entry *dst,
 			      struct flowi *fl,
@@ -540,6 +555,32 @@ static int tcp_v6_send_synack(const struct sock *sk, struct dst_entry *dst,
 		opt = ireq->ipv6_opt;
 		if (!opt)
 			opt = rcu_dereference(np->opt);
+
+		if (DTRACE_TCP_ENABLED(send)) {
+			struct ipv6hdr ip6h;
+
+			dtrace_tcp_build_ipv6hdr(&ireq->ir_v6_loc_addr,
+						 &ireq->ir_v6_rmt_addr, &ip6h);
+
+			/* Do not supply tcp sk - addresses/ports are not
+			 * committed yet - instead translators will fill them
+			 * in from IP/TCP data.
+			 */
+			DTRACE_TCP_NOCHECK(send,
+					   struct sk_buff * :  pktinfo_t *,
+					   NULL,
+					   struct sock * : csinfo_t *, sk,
+					   __dtrace_tcp_void_ip_t * :
+					   ipinfo_t *, &ip6h,
+					   struct tcp_sock * : tcpsinfo_t *,
+					   NULL,
+					   struct tcphdr * : tcpinfo_t *,
+					   tcp_hdr(skb),
+					   int : tcplsinfo_t *, TCP_LISTEN,
+					   int, TCP_LISTEN,
+					   int, DTRACE_NET_PROBE_OUTBOUND);
+		}
+
 		err = ip6_xmit(sk, skb, fl6, sk->sk_mark, opt,
 			       tclass, sk->sk_priority);
 		rcu_read_unlock();
@@ -969,6 +1010,49 @@ static void tcp_v6_send_response(const struct sock *sk, struct sk_buff *skb, u32
 	dst = ip6_dst_lookup_flow(sock_net(ctl_sk), ctl_sk, &fl6, NULL);
 	if (!IS_ERR(dst)) {
 		skb_dst_set(buff, dst);
+
+		if (DTRACE_TCP_ENABLED(send) ||
+		    DTRACE_TCP_ENABLED(accept__refused)) {
+			struct ipv6hdr ip6h;
+
+			dtrace_tcp_build_ipv6hdr(&fl6.saddr, &fl6.daddr,
+						 &ip6h);
+
+			/* Do not supply tcp sk - addresses/ports are not
+			 * committed yet - instead translators will fill them
+			 * in from IP/TCP data.
+			 */
+			DTRACE_TCP_NOCHECK(send,
+					   struct sk_buff * :  pktinfo_t *,
+					   NULL,
+					   struct sock * : csinfo_t *, NULL,
+					   __dtrace_tcp_void_ip_t * :
+					   ipinfo_t *, &ip6h,
+					   struct tcp_sock * : tcpsinfo_t *,
+					   NULL,
+					   struct tcphdr * : tcpinfo_t *, t1,
+					   int : tcplsinfo_t *, TCP_CLOSE,
+					   int, TCP_CLOSE,
+					   int, DTRACE_NET_PROBE_OUTBOUND);
+			if (rst && th->syn && th->ack == 0)
+				DTRACE_TCP_NOCHECK(accept__refused,
+						   struct sk_buff * :
+						   pktinfo_t *, NULL,
+						   struct sock * : csinfo_t *,
+						   NULL,
+						   __dtrace_tcp_void_ip_t * :
+						   ipinfo_t *, &ip6h,
+						   struct tcp_sock * :
+						   tcpsinfo_t *, NULL,
+						   struct tcphdr * :
+						   tcpinfo_t *, t1,
+						   int : tcplsinfo_t *,
+						   TCP_CLOSE,
+						   int, TCP_CLOSE,
+						   int,
+						   DTRACE_NET_PROBE_OUTBOUND);
+		}
+
 		ip6_xmit(ctl_sk, buff, &fl6, fl6.flowi6_mark, NULL,
 			 tclass & ~INET_ECN_MASK, priority);
 		TCP_INC_STATS(net, TCP_MIB_OUTSEGS);
@@ -1583,7 +1667,7 @@ INDIRECT_CALLABLE_SCOPE int tcp_v6_rcv(struct sk_buff *skb)
 	const struct tcphdr *th;
 	const struct ipv6hdr *hdr;
 	bool refcounted;
-	struct sock *sk;
+	struct sock *sk = NULL;
 	int ret;
 	struct net *net = dev_net(skb->dev);
 
@@ -1618,6 +1702,15 @@ INDIRECT_CALLABLE_SCOPE int tcp_v6_rcv(struct sk_buff *skb)
 	if (!sk)
 		goto no_tcp_socket;
 
+	DTRACE_TCP(receive,
+		   struct sk_buff * :  pktinfo_t *, skb,
+		   struct sock * : csinfo_t *, sk,
+		   __dtrace_tcp_void_ip_t * : ipinfo_t *, hdr,
+		   struct tcp_sock * : tcpsinfo_t *, tcp_sk(sk),
+		   struct tcphdr * : tcpinfo_t *, th,
+		   int : tcplsinfo_t *, sk ? sk->sk_state : TCP_CLOSE,
+		   int, sk ? sk->sk_state : TCP_CLOSE,
+		   int, DTRACE_NET_PROBE_INBOUND);
 process:
 	if (sk->sk_state == TCP_TIME_WAIT)
 		goto do_time_wait;
@@ -1737,6 +1830,18 @@ INDIRECT_CALLABLE_SCOPE int tcp_v6_rcv(struct sk_buff *skb)
 	}
 
 discard_it:
+	if (DTRACE_TCP_ENABLED(receive) && skb->pkt_type == PACKET_HOST)
+		DTRACE_TCP_NOCHECK(receive,
+				   struct sk_buff * :  pktinfo_t *, skb,
+				   struct sock * : csinfo_t *, sk,
+				   __dtrace_tcp_void_ip_t * : ipinfo_t *,
+				   ipv6_hdr(skb),
+				   struct tcp_sock * : tcpsinfo_t *, tcp_sk(sk),
+				   struct tcphdr * : tcpinfo_t *, tcp_hdr(skb),
+				   int : tcplsinfo_t *,
+				   sk ? sk->sk_state : TCP_CLOSE,
+				   int, sk ? sk->sk_state : TCP_CLOSE,
+				   int, DTRACE_NET_PROBE_INBOUND);
 	kfree_skb(skb);
 	return 0;
 
diff --git a/net/ipv6/udp.c b/net/ipv6/udp.c
index 29d9691359b9..996afe0fb057 100644
--- a/net/ipv6/udp.c
+++ b/net/ipv6/udp.c
@@ -51,6 +51,7 @@
 
 #include <linux/proc_fs.h>
 #include <linux/seq_file.h>
+#include <linux/sdt.h>
 #include <trace/events/skb.h>
 #include "udp_impl.h"
 
@@ -386,8 +387,15 @@ int udpv6_recvmsg(struct sock *sk, struct msghdr *msg, size_t len,
 		kfree_skb(skb);
 		return err;
 	}
-	if (!peeking)
+	if (!peeking) {
 		SNMP_INC_STATS(mib, UDP_MIB_INDATAGRAMS);
+		DTRACE_UDP(receive,
+			   struct sk_buff * :  pktinfo_t *, skb,
+			   struct sock * : csinfo_t *, sk,
+			   void_ip_t * : ipinfo_t *, ip_hdr(skb),
+			   struct udp_sock * : udpsinfo_t *, udp_sk(sk),
+			   struct udphdr * : udpinfo_t *, udp_hdr(skb));
+	}
 
 	sock_recv_ts_and_drops(msg, sk, skb);
 
@@ -685,6 +693,15 @@ static int udpv6_queue_rcv_one_skb(struct sock *sk, struct sk_buff *skb)
 
 			ret = encap_rcv(sk, skb);
 			if (ret <= 0) {
+				DTRACE_UDP(receive,
+					   struct sk_buff * :  pktinfo_t *, skb,
+					   struct sock * : csinfo_t *, sk,
+					   void_ip_t * : ipinfo_t *,
+					   ip_hdr(skb),
+					   struct udp_sock * : udpsinfo_t *,
+					   udp_sk(sk),
+					   struct udphdr * : udpinfo_t *,
+					   udp_hdr(skb));
 				__UDP_INC_STATS(sock_net(sk),
 						UDP_MIB_INDATAGRAMS,
 						is_udplite);
@@ -1238,6 +1255,13 @@ static int udp_v6_send_skb(struct sk_buff *skb, struct flowi6 *fl6,
 			err = 0;
 		}
 	} else {
+		DTRACE_UDP(send,
+			   struct sk_buff * :  pktinfo_t *, skb,
+			   struct sock * : csinfo_t *, sk,
+			   void_ip_t * : ipinfo_t *, ip_hdr(skb),
+			   struct udp_sock * : udpsinfo_t *, udp_sk(sk),
+			   struct udphdr * : udpinfo_t *, uh);
+
 		UDP6_INC_STATS(sock_net(sk),
 			       UDP_MIB_OUTDATAGRAMS, is_udplite);
 	}
-- 
2.30.0

